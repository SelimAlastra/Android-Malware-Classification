import shodan
from shodan import Shodan
from rich.console import Console
import pickle
from urllib.parse import urlparse
from tqdm import tqdm
from socket import gethostbyname
import socket
import shodan
import concurrent.futures
import ipaddress
from threading import Lock
import time

lock = Lock()
console = Console()
shodanApi = Shodan('GlckC84d1kkLi5KS5kvGEFuVaog4HfkP')

def get_urls_hostnames(X_2D):
        
    console.log("Parsing URLS ...")
    urls_hostnames = set()
    
    total_urls = len(X_2D)
    progress_bar = tqdm(total=total_urls, desc="PARSING-URLS")

    for urls in X_2D:
        progress_bar.update(1)
        for url in urls:
            try:
                parsed_url = urlparse(url)
                urls_hostnames.add(parsed_url.hostname)
            except ValueError:
                continue

    return urls_hostnames
  
def is_ip_address(s):
    try:
        ipaddress.ip_address(s)
        return True
    except ValueError:
        return False

def extract_shodan_features(urlsHostnames):
    
    total_urls = len(urlsHostnames)
    console.log("Loading urlsHostnames ...")

    try:
        with open("url_shodan_features.pickle", "rb") as f:
            urlShodanFeaturesDict = pickle.load(f)
    except FileNotFoundError:
        urlShodanFeaturesDict = {}
    counter = 0

    start_time = time.time()
    print_interval = 30  # Print interval in seconds

    def process_url(url):

        nonlocal counter
        shodan_info = None

        if url and url not in urlShodanFeaturesDict:
            try:
                ip = url if is_ip_address(url) else gethostbyname(url)

                try:                    
                    shodan_info =  shodanApi.host(ip, minify=True)
                except shodan.exception.APIError:
                    shodan_info = 'API-ERROR'

            except socket.gaierror as e:
                shodan_info = 'FAILED-TO-GET-IP'
            except UnicodeError:
                shodan_info = "UNICODE-ERROR"

        counter += 1
        progress_bar.update(1)

        return url, shodan_info

    with concurrent.futures.ThreadPoolExecutor() as executor:
        progress_bar = tqdm(total=total_urls, desc="SHODAN-FEATURE-EXTRACTION")
        futures = [executor.submit(process_url, url) for url in urlsHostnames]
        
        for future in concurrent.futures.as_completed(futures):
            url, shodan_info = future.result()
            
            if shodan_info is not None:
                urlShodanFeaturesDict[url] = shodan_info

            elapsed_time = time.time() - start_time

            if elapsed_time >= print_interval:
                print("30 seconds have passed")
                with open("url_shodan_features.pickle", "wb") as f:
                    pickle.dump(urlShodanFeaturesDict, f)

                start_time = time.time()  # Reset the start time

    with open("url_shodan_features.pickle", "wb") as f:
        pickle.dump(urlShodanFeaturesDict, f)

    return urlShodanFeaturesDict

def update_shodan_features(urlsHostnames):
    
    total_urls = len(urlsHostnames)
    console.log("Loading urlsHostnames ...")

    urlShodanFeaturesDict = {}

    counter = 0

    def process_url(url):

        nonlocal counter
        shodan_info = None

        if url and url not in urlShodanFeaturesDict:
            try:
                ip = url if is_ip_address(url) else gethostbyname(url)

                try:                    
                    shodan_info =  shodanApi.host(ip, minify=True)
                except shodan.exception.APIError:
                    shodan_info = 'API-ERROR'

            except socket.gaierror as e:
                shodan_info = 'FAILED-TO-GET-IP'
            except UnicodeError:
                shodan_info = "UNICODE-ERROR"

        counter += 1
        progress_bar.update(1)

        return url, shodan_info

    with concurrent.futures.ThreadPoolExecutor() as executor:
        progress_bar = tqdm(total=total_urls, desc="SHODAN-FEATURE-EXTRACTION")
        futures = [executor.submit(process_url, url) for url in urlsHostnames]
        
        for future in concurrent.futures.as_completed(futures):
            url, shodan_info = future.result()
            
            if shodan_info is not None:
                urlShodanFeaturesDict[url] = shodan_info

    return urlShodanFeaturesDict

def categorize_shodan_data(shodan_data):
    category_counts = {"API-ERROR": 0, "FAILED-TO-GET-IP": 0, "IP-NOT_FOUND": 0, "NULL": 0, "VALID": 0}
    categorized_data = {"API-ERROR": set(), "FAILED-TO-GET-IP": set(), "IP-NOT_FOUND": set(), "NULL": set(), "VALID": {}}

    for url, shodan_info in shodan_data.items():
        if shodan_info in ["API-ERROR", "FAILED-TO-GET-IP", "IP-NOT_FOUND"]:
            category_counts[shodan_info] += 1
            categorized_data[shodan_info].add(url)
        elif isinstance(shodan_info, dict) and all(val is None for val in shodan_info.values()):
            category_counts["NULL"] += 1
            categorized_data["NULL"].add(url)
        else:
            category_counts["VALID"] += 1
            categorized_data["VALID"][url] = shodan_info

    return category_counts, categorized_data

def print_category_counts(category_counts):
    for category, count in category_counts.items():
        print(f"{category.lower()}: {count}")

with open("urls_hostnames.pickle", "rb") as f:
    urls_hostnames = pickle.load(f)

shodanFeatures = extract_shodan_features(urls_hostnames)

valid_count_diff = 1  # Initialize with a non-zero value for the first iteration

while valid_count_diff > 0:
    # Categorize shodan data and print category counts
    category_counts, categorized_data = categorize_shodan_data(shodanFeatures)
    print_category_counts(category_counts)
    
    valid_count_diff = 0

    # Update urls_hostnames with unresolved urls and update valid_count_diff
    unresolved_urls_hostnames = categorized_data['FAILED-TO-GET-IP'].union(categorized_data['API-ERROR'])
    newShodanFeatures = update_shodan_features(unresolved_urls_hostnames)

    shodanFeatures.update(newShodanFeatures)
    category_counts_new, _ = categorize_shodan_data(shodanFeatures)

    valid_count_diff = category_counts_new["VALID"] - category_counts["VALID"]  # Update the valid_count_diff
    console.log("Added " + str(valid_count_diff) + " valid urls ...")

    # Save the unresolved urls and the final shodan_features to files
    with open("unresolved_shodan_urls.pickle", "wb") as f:
        pickle.dump(unresolved_urls_hostnames, f)
    with open("url_shodan_features.pickle", "wb") as f:
        pickle.dump(shodanFeatures, f)

