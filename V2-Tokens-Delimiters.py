import json
import pickle
import datetime
import numpy as np
from rich.console import Console
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pickle
from sklearn.svm import LinearSVC
from urllib.parse import urlparse
from sklearn.ensemble import RandomForestClassifier
import re

console = Console()

console.log("Running Tokens 1 ...")

def load_dataset(dataset_path, use_cache=True):

    if use_cache:
        console.log('Trying to load dataset from cache')
        try:
            with open('cache.pkl', 'rb') as f:
                console.log('Loaded dataset from cache')
                return pickle.load(f)
        except FileNotFoundError:
            pass

    console.log(f'Loading dataset from {dataset_path}')

    with open('{}-X-updated.json'.format(dataset_path), 'r') as f:
        X = json.load(f)
    # if not shas:
    #     [o.pop('sha256') for o in X]

    console.log('Loading labels...')
    with open('{}-y-updated.json'.format(dataset_path), 'rt') as f:
        y = json.load(f)
    # if 'apg' not in dataset_path:
    #     y = [o[0] for o in y]

    console.log('Loading timestamps...')
    with open('{}-meta-updated.json'.format(dataset_path), 'rt') as f:
        T = json.load(f)
    T = [o['dex_date'] for o in T]
    T = [datetime.datetime.strptime(o, '%Y-%m-%dT%H:%M:%S') if "T" in o
             else datetime.datetime.strptime(o, '%Y-%m-%d %H:%M:%S') for o in T]

    time_index = {}
    for i in range(len(T)):
        t = T[i]
        if t.year not in time_index:
            time_index[t.year] = {}
        if t.month not in time_index[t.year]:
            time_index[t.year][t.month] = []
        time_index[t.year][t.month].append(i)
    
    data = X, y, time_index, T
    with open('cache.pkl', 'wb') as f:
        pickle.dump(data, f)

    return data

def replace_underscores(X):
    new_X = []
    for d in X:
        new_d = {}
        for k, v in d.items():
            new_k = k.replace('_', '.')
            new_d[new_k] = v
        new_X.append(new_d)
    return new_X

def filterData(X, y, filter_key):

    console.log(f"Filtering data with filter_key: ({filter_key})....")
    filtered_X = []
    filtered_y = []
    prefix_len = len(filter_key)

    for i in range(len(X)):
        if any(k.startswith(filter_key) for k in X[i]):
            filtered_dict = {k[prefix_len:]: v for k, v in X[i].items() if k.startswith(filter_key)}
            filtered_X.append(filtered_dict)
            filtered_y.append(y[i])

    filtered_X = replace_underscores(filtered_X)

    return filtered_X, filtered_y

def dict_to_2d_list(X_dict):

    console.log('Converting into X into 2D list ...')

    X_list = []
    for x_dict in X_dict:
        x_list = list(x_dict.keys())
        X_list.append(x_list)
    return X_list

def vectorize_DictVectorizer(X,y):

    console.log('Using DictVectorizer...')

    # Convert to numpy array
    vec =  DictVectorizer()
    X = vec.fit_transform(X).astype("float32")
    y = np.asarray(y)
    feature_names = vec.get_feature_names_out()

    return X,y,feature_names

def evaluate_classifier(clf, X_test, y_test):
    """
    Evaluates a trained classifier on a given testing set.

    Args:
        clf: The trained classifier.
        X_test: The testing set features.
        y_test: The testing set labels.

    Returns:
        A dictionary of the performance metrics including testing accuracy, false positive rate,
        false negative rate, and F1 score.
    """

    console.log('Evaluating Classifier...')

    # Test the classifier on the testing set
    y_pred = clf.predict(X_test)
    accuracy = clf.score(X_test, y_test)
    f1score = f1_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    false_positive_rate = fp / (fp + tn)
    false_negative_rate = fn / (fn + tp)

    print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score)

    results = {
        "accuracy": accuracy,
        "false_positive_rate": false_positive_rate,
        "false_negative_rate": false_negative_rate,
        "f1_score": f1score,
    }

    return results

def print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score):
    """
    Prints the evaluation metrics of a classifier.

    Args:
        accuracy (float): The testing accuracy of the classifier.
        false_positive_rate (float): The false positive rate of the classifier.
        false_negative_rate (float): The false negative rate of the classifier.
        f1score (float): The F1 score of the classifier.
    """
    print(f'Testing accuracy: {accuracy}')
    print(f'False Positive rate: {false_positive_rate}')
    print(f'False Negative rate: {false_negative_rate}')
    print(f'F1 Score: {f1score}')

def train_linear_svc(vectorized_X, y_vectorized, feature_names):

    console.log('Training SVC Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorized_X, y_vectorized, test_size=0.1, random_state=42)

    # Train the LinearSVC classifier
    clf = LinearSVC(max_iter=10000)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    console.log('Finding most important features..')

    coef = clf.coef_[0]
    top_indices = np.argsort(np.abs(coef))[::-1][:10]
    top_features = [feature_names[i] for i in top_indices]

    print(f"Top 10 features: {top_features}")
    print(f"Feature weights: {coef[top_indices]}")

    return clf, X_train, X_test

def train_random_forest(vectorized_X, y_vectorized, feature_names, n_jobs=-1):

    console.log('Training Random Forest Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorized_X, y_vectorized, test_size=0.1, random_state=42)

    # Train the Random Forest classifier
    clf = RandomForestClassifier(n_jobs=n_jobs, random_state=42)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    console.log('Finding most important features..')

    feature_importances = clf.feature_importances_
    top_indices = np.argsort(feature_importances)[::-1][:10]
    top_features = [feature_names[i] for i in top_indices]

    print(f"Top 10 features: {top_features}")
    print(f"Feature importances: {feature_importances[top_indices]}")

    return clf, X_train, X_test

def subsample_program(X, y, fraction):
    
    console.log("Subsampling data ...")

    split_index = int(len(X) * fraction)

    X_filtered = X[:split_index]
    y_filtered = y[:split_index]

    return X_filtered, y_filtered

def get_tokens(url):
    special_chars = r"/&:?=#.-+%@!$*'(),;=[]"

    # Split the URL using the special characters as delimiters
    tokens = re.split("[" + re.escape(special_chars) + "]", url)

    # Remove empty tokens
    tokens = [token for token in tokens if token]

    return tokens

def tokenize_urls(X_2D):

    X_features = []

    for urls in X_2D:

        features = {}

        for url in urls:

            try: 

                parsed_url = urlparse(url)

                divided_url = get_tokens(url)

                for feature in divided_url:
                    if feature in features:
                        features[feature] += 1
                    else:
                        features[feature] = 1

            except ValueError as e:

                continue

        X_features.append(features)

    return X_features

def vectorize_features(X_features):

    console.log("Vectorizing ...")

    # Create an instance of DictVectorizer
    vectorizer = DictVectorizer()

    X_vectorized = vectorizer.fit_transform(X_features)

    feature_names = vectorizer.get_feature_names_out()

    return X_vectorized, feature_names

X, y, time_index, T = load_dataset("dataset/extended-features")

# Check if the filtered data is already saved to a file
try:
    with open("filtered_data.pickle", "rb") as f:
        console.log("Loaded filtered data from cache")
        X_filtered, y_filtered = pickle.load(f)
except FileNotFoundError:
    # if the file is not found, filter the data and save it to a file
    X_filtered, y_filtered = filterData(X, y, "urls::")
    with open("filtered_data.pickle", "wb") as f:
        pickle.dump((X_filtered, y_filtered), f)

# Subsample data
X_filtered, y_filtered = subsample_program(X_filtered, y_filtered, 1)

X_2D = dict_to_2d_list(replace_underscores(X_filtered))

X_features = tokenize_urls(X_2D)

X_vectorized, feature_names, vectoriser = vectorize_features(X_features)

# svc_clf = train_linear_svc(X_vectorized, y_filtered, feature_names)

# random_forest_clf = train_random_forest(X_vectorized, y_filtered, feature_names)

# with open('tk3_f_svc_clf.pkl', 'wb') as file:
#     pickle.dump(svc_clf, file)

# with open('tk3_f_random_forest_clf.pkl', 'wb') as file:
#     pickle.dump(random_forest_clf, file)


