import json
import pickle
import datetime
import numpy as np
from rich.console import Console
from sklearn.feature_extraction import DictVectorizer
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pickle
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

console = Console()

def load_dataset(dataset_path, use_cache=True):

    if use_cache:
        console.log('Trying to load dataset from cache')
        try:
            with open('cache.pkl', 'rb') as f:
                console.log('Loaded dataset from cache')
                return pickle.load(f)
        except FileNotFoundError:
            pass

    console.log(f'Loading dataset from {dataset_path}')

    with open('{}-X-updated.json'.format(dataset_path), 'r') as f:
        X = json.load(f)
    # if not shas:
    #     [o.pop('sha256') for o in X]

    console.log('Loading labels...')
    with open('{}-y-updated.json'.format(dataset_path), 'rt') as f:
        y = json.load(f)
    # if 'apg' not in dataset_path:
    #     y = [o[0] for o in y]

    console.log('Loading timestamps...')
    with open('{}-meta-updated.json'.format(dataset_path), 'rt') as f:
        T = json.load(f)
    T = [o['dex_date'] for o in T]
    T = [datetime.datetime.strptime(o, '%Y-%m-%dT%H:%M:%S') if "T" in o
             else datetime.datetime.strptime(o, '%Y-%m-%d %H:%M:%S') for o in T]

    time_index = {}
    for i in range(len(T)):
        t = T[i]
        if t.year not in time_index:
            time_index[t.year] = {}
        if t.month not in time_index[t.year]:
            time_index[t.year][t.month] = []
        time_index[t.year][t.month].append(i)
    
    data = X, y, time_index, T
    with open('cache.pkl', 'wb') as f:
        pickle.dump(data, f)

    return data

def replace_underscores(X):
    new_X = []
    for d in X:
        new_d = {}
        for k, v in d.items():
            new_k = k.replace('_', '.')
            new_d[new_k] = v
        new_X.append(new_d)
    return new_X

def filterData(X, y, filter_key):

    console.log(f"Filtering data with filter_key: ({filter_key})....")
    filtered_X = []
    filtered_y = []
    prefix_len = len(filter_key)

    for i in range(len(X)):
        if any(k.startswith(filter_key) for k in X[i]):
            filtered_dict = {k[prefix_len:]: v for k, v in X[i].items() if k.startswith(filter_key)}
            filtered_X.append(filtered_dict)
            filtered_y.append(y[i])

    # filtered_X = replace_underscores(filtered_X)

    return filtered_X, filtered_y

def vectorize_DictVectorizer(X,y):

    console.log('Using DictVectorizer...')

    # Convert to numpy array
    vec =  DictVectorizer()
    X = vec.fit_transform(X).astype("float32")
    # y = np.asarray(y)
    feature_names = vec.get_feature_names_out()

    return X,y,feature_names

def evaluate_classifier(clf, X_test, y_test):
    """
    Evaluates a trained classifier on a given testing set.

    Args:
        clf: The trained classifier.
        X_test: The testing set features.
        y_test: The testing set labels.

    Returns:
        A dictionary of the performance metrics including testing accuracy, false positive rate,
        false negative rate, and F1 score.
    """

    console.log('Evaluating Classifier...')

    # Test the classifier on the testing set
    y_pred = clf.predict(X_test)
    accuracy = clf.score(X_test, y_test)
    f1score = f1_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    false_positive_rate = fp / (fp + tn)
    false_negative_rate = fn / (fn + tp)

    print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score)

    results = {
        "accuracy": accuracy,
        "false_positive_rate": false_positive_rate,
        "false_negative_rate": false_negative_rate,
        "f1_score": f1score,
    }

    return results

def print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score):
    """
    Prints the evaluation metrics of a classifier.

    Args:
        accuracy (float): The testing accuracy of the classifier.
        false_positive_rate (float): The false positive rate of the classifier.
        false_negative_rate (float): The false negative rate of the classifier.
        f1score (float): The F1 score of the classifier.
    """
    print(f'Testing accuracy: {accuracy}')
    print(f'False Positive rate: {false_positive_rate}')
    print(f'False Negative rate: {false_negative_rate}')
    print(f'F1 Score: {f1score}')

def train_linear_svc(vectorized_X, y_vectorized, feature_names):

    console.log('Training SVC Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorized_X, y_vectorized, test_size=0.1, random_state=42)

    # Train the LinearSVC classifier
    clf = LinearSVC(max_iter=1000)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    console.log('Finding most important features..')

    coef = clf.coef_[0]
    top_indices = np.argsort(np.abs(coef))[::-1][:10]
    top_features = [feature_names[i] for i in top_indices]

    print(f"Top 10 features: {top_features}")
    print(f"Feature weights: {coef[top_indices]}")

    return clf

def train_random_forest(vectorized_X, y_vectorized, feature_names, n_jobs=-1):

    console.log('Training Random Forest Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorized_X, y_vectorized, test_size=0.1, random_state=42)

    # Train the Random Forest classifier
    clf = RandomForestClassifier(n_jobs=n_jobs, random_state=42)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    console.log('Finding most important features..')

    feature_importances = clf.feature_importances_
    top_indices = np.argsort(feature_importances)[::-1][:10]
    top_features = [feature_names[i] for i in top_indices]

    print(f"Top 10 features: {top_features}")
    print(f"Feature importances: {feature_importances[top_indices]}")

    return clf, X_train, X_test

def train_logistic_regression(vectorized_X, y_vectorized):

    console.log('Training Logistic Regression Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorized_X, y_vectorized, test_size=0.1, random_state=42)

    # Train the logistic regression classifier
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    return clf

X, y, time_index, T = load_dataset("dataset/extended-features")

X_filtered, y_filtered = filterData(X, y, "urls::")
X_vectorized, y_vectorized , feature_names= vectorize_DictVectorizer(X_filtered, y_filtered)

# train_linear_svc(X_vectorized, y_vectorized, feature_names)
# train_random_forest(X_vectorized, y_vectorized, feature_names)

