from rich.console import Console
import pickle
from urllib.parse import urlparse
from tqdm import tqdm
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction import DictVectorizer
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

console = Console()

with open("urls_hostnames.pickle", "rb") as f:
    urls_hostnames = pickle.load(f)

urls_hostnames.discard(None)

with open("url_shodan_features.pickle", "rb") as f:
    shodanFeatures = pickle.load(f)

with open("url_whois_features.pickle", "rb") as f:
    whoIsFeatures = pickle.load(f)

def replace_underscores(X):
    new_X = []
    for d in X:
        new_d = {}
        for k, v in d.items():
            new_k = k.replace('_', '.')
            new_d[new_k] = v
        new_X.append(new_d)
    return new_X

def dict_to_2d_list(X_dict):

    console.log('Converting into X into 2D list ...')

    X_list = []
    for x_dict in X_dict:
        x_list = list(x_dict.keys())
        X_list.append(x_list)
    return X_list

def getUrlShodanFeaturesDict(shodanFeatures, url):

    features = {}
    position_info = {}

    errCodes = ["API-ERROR","FAILED-TO-GET-IP","UNICODE-ERROR", "EXCEPTION"] 

    keys = ['region_code', 'tags', 'ip', 'area_code', 'domains', 
            'hostnames', 'country_code', 'org', 'data', 'asn', 'city', 
            'latitude', 'isp', 'longitude', 'last_update', 'country_name', 
            'ip_str', 'os', 'ports', 'vulns']

    shodanInfo = shodanFeatures[url]

    for errCode in errCodes:
        if shodanInfo == errCode:
            return {'shodan-'+errCode:1}, None
        
    for key in keys:
        value = shodanInfo.get(key)
        if value is not None:
            if key == 'latitude':
                position_info['latitude'] = value
            elif key == 'longitude':
                position_info['longitude'] = value
            if isinstance(value, list):
                for element in value:
                    features[key + "::" + str(element)] = 1
            else:
                features[key + "-shodan-::" + str(value)] = 1
        else: features[key + "-shodan-::NotFound"] = 1

    # features.update(position_info)

    return features, position_info

def getUrlWhoIsFeaturesDict(whoIsFeatures, url):

    features = {}
    date_info = {}

    whoIsInfo = whoIsFeatures[url]
    # print(whoIsInfo)

    errCodes = ["NOT-REGISTRED","REGEX-ERROR","EXCEPTION"] 
    
    for errCode in errCodes:
        if whoIsInfo == errCode:
            return {'whois'+errCode:1}, None

    keys = ["domain_name", "registrar", "referral_url", "dnssec", "name", 
            "org", "address", "city", "state", "registrant_postal_code", 
            "country", "name_servers", "emails", "registrar_country",
            ]
    
    for key in keys:
        value = whoIsInfo.get(key)
        if value is not None:
            if isinstance(value, list):
                for element in value:
                    features[key + "::" + element] = 1
            else:
                features[key + "-whois-::" + value] = 1
        else: features[key + "-whois-::NotFound"] = 1

    date_keys = ["creation_date", "updated_date", "expiration_date", "last_modified"]
    nowDate = datetime.now().timestamp()

    for date_key in date_keys:
        value = whoIsInfo.get(date_key)
        if value is not None:
            if isinstance(value, list):
                try:
                    date_info[date_key] = value[0].timestamp()
                except Exception as e:
                    # print(e)
                    if len(value) > 1: date_info[date_key] = value[1].timestamp()
                    
            elif isinstance(value, str):
                try:
                    # Use split to divide the string into two parts at "(GMT"
                    # We will only use the first part, which we get by accessing [0]
                    value = value.split("(GMT")[0].strip()

                    # Now value is "2004-07-23 02:18:42 "
                    # Parse it with datetime.strptime
                    date_format = "%Y-%m-%d %H:%M:%S"
                    date_object = datetime.strptime(value, date_format)
                    date_info[date_key] = date_object.timestamp()
                except Exception as e:
                    continue
            else:
                date_info[date_key] = value.timestamp()

    if "creation_date" in date_info and "expiration_date" in date_info:
        date_info["whois-lifespan"] = date_info["expiration_date"] - date_info["creation_date"]
    
    if "creation_date" in date_info:
        date_info["whois-age"] = nowDate - date_info["creation_date"]
    
    if "expiration_date" in date_info:
        date_info["whois-lifeRemaining"] = date_info["expiration_date"] - nowDate
    
    if "last_modified" in date_info:
        date_info["whois-last_modified"] = date_info["last_modified"]

    # features.update(date_info)

    return features, date_info

def getUrlsHostFeatures(shodanFeatures, whoIsFeatures, urls):

    features = {}
    numerical_info = []
 
    for url in urls:

        shodan_features, position_info = getUrlShodanFeaturesDict(shodanFeatures, url)
        whois_features, date_info = getUrlWhoIsFeaturesDict(whoIsFeatures, url)

        numerical_info.append(position_info)
        numerical_info.append(date_info)

        for key, value in shodan_features.items():
            if key in features:
                features[key] += value
            else:
                features[key] = value

        for key, value in whois_features.items():
            if key in features:
                features[key] += value
            else:
                features[key] = value

    numericalValuesKeys = ["creation_date", "updated_date", "expiration_date",
                            "last_modified", "latitude", "longitude", "whois-lifespan",
                            "whois-age", "whois-lifeRemaining", "whois-last_modified"]

    for key in numericalValuesKeys:
        values = [d.get(key) for d in numerical_info if d is not None and key in d]
        count = len(values)
        if count > 0:
            average = sum(values) / count
            features[key] = average

    return features

def vectorize_features(X_features):

    console.log("Vectorizing ...")

    # Create an instance of DictVectorizer
    vectorizer = DictVectorizer()

    X_vectorized = vectorizer.fit_transform(X_features)

    feature_names = vectorizer.get_feature_names_out()

    return X_vectorized, feature_names, vectorizer

def evaluate_classifier(clf, X_test, y_test):
    """
    Evaluates a trained classifier on a given testing set.

    Args:
        clf: The trained classifier.
        X_test: The testing set features.
        y_test: The testing set labels.

    Returns:
        A dictionary of the performance metrics including testing accuracy, false positive rate,
        false negative rate, and F1 score.
    """

    console.log('Evaluating Classifier...')

    # Test the classifier on the testing set
    y_pred = clf.predict(X_test)
    accuracy = clf.score(X_test, y_test)
    f1score = f1_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    false_positive_rate = fp / (fp + tn)
    false_negative_rate = fn / (fn + tp)

    print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score)

    results = {
        "accuracy": accuracy,
        "false_positive_rate": false_positive_rate,
        "false_negative_rate": false_negative_rate,
        "f1_score": f1score,
    }

    return results

def print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score):
    """
    Prints the evaluation metrics of a classifier.

    Args:
        accuracy (float): The testing accuracy of the classifier.
        false_positive_rate (float): The false positive rate of the classifier.
        false_negative_rate (float): The false negative rate of the classifier.
        f1score (float): The F1 score of the classifier.
    """
    print(f'Testing accuracy: {accuracy}')
    print(f'False Positive rate: {false_positive_rate}')
    print(f'False Negative rate: {false_negative_rate}')
    print(f'F1 Score: {f1score}')

def train_random_forest(vectorized_X, y_vectorized, feature_names, n_jobs=-1):

    console.log('Training Random Forest Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorized_X, y_vectorized, test_size=0.1, random_state=21)

    # Train the Random Forest classifier
    clf = RandomForestClassifier(n_jobs=n_jobs, random_state=42)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    console.log('Finding most important features..')

    feature_importances = clf.feature_importances_
    top_indices = np.argsort(feature_importances)[::-1][:10]
    top_features = [feature_names[i] for i in top_indices]

    print(f"Top 10 features: {top_features}")
    print(f"Feature importances: {feature_importances[top_indices]}")

    return clf, X_train, X_test, y_train, y_test

def train_linear_svc(vectorised_X, y_vectorised, feature_names):

    console.log('Training SVC Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorised_X, y_vectorised, test_size=0.1, random_state=42)

    # Train the LinearSVC classifier
    clf = LinearSVC(max_iter=10000)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    console.log('Finding most important features..')

    coef = clf.coef_[0]
    top_indices = np.argsort(np.abs(coef))[::-1][:10]
    top_features = [feature_names[i] for i in top_indices]

    print(f"Top 10 features: {top_features}")
    print(f"Feature weights: {coef[top_indices]}")

    return clf, X_train, X_test, y_train, y_test

with open("filtered_data.pickle", "rb") as f:
    console.log("Loaded filtered data from cache")
    X_filtered, y_filtered = pickle.load(f)

X_2D = dict_to_2d_list(replace_underscores(X_filtered))

console.log("Extracting hostbased features ...")

X_features = []

with tqdm(total=len(X_2D), desc="Processing URLs") as pbar:
    
    for urls in X_2D:
        collected_urls_hostnames = set()
        for url in urls:
            try:
                url_hostname = urlparse(url).hostname
                if url_hostname in urls_hostnames: collected_urls_hostnames.add(url_hostname)
            except Exception:
                continue
        
        hostBasedInfo = getUrlsHostFeatures(shodanFeatures, whoIsFeatures, collected_urls_hostnames)
        X_features.append(hostBasedInfo)
        pbar.update(1)

X_vectorized, feature_names, vectoriser = vectorize_features(X_features)

# random_forest_clf, X_train, X_test, y_train, y_test = train_random_forest(X_vectorized, y_filtered, feature_names)

# svc_clf, X_train, X_test, y_train, y_test = train_linear_svc(X_vectorized, y_filtered, feature_names)

# with open('hostBased_random_forest_clf.pkl', 'wb') as file:
#     pickle.dump(random_forest_clf, file)

# with open('hostBased_svc_clf.pkl', 'wb') as file:
#     pickle.dump(svc_clf, file)

# with open('hostBased_random_forest_vectoriser.pkl', 'wb') as file:
#     pickle.dump(vectoriser, file)

# with open('hostBased_svc_vectoriser.pkl', 'wb') as file:
#     pickle.dump(vectoriser, file)
