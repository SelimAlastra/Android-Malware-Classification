import whois
from shodan import Shodan
from rich.console import Console
import pickle
from urllib.parse import urlparse
from tqdm import tqdm
from socket import gethostbyname
import re
import concurrent.futures
import os
import ipaddress
from threading import Lock
import itertools

lock = Lock()
console = Console()

def get_urls_hostnames(X_2D):
        
    console.log("Parsing URLS ...")
    urls_hostnames = set()
    
    total_urls = len(X_2D)
    progress_bar = tqdm(total=total_urls, desc="PARSING-URLS")

    for urls in X_2D:
        progress_bar.update(1)
        for url in urls:
            try:
                parsed_url = urlparse(url)
                urls_hostnames.add(parsed_url.hostname)
            except ValueError:
                continue

    return urls_hostnames
  
def is_ip_address(s):
    try:
        ipaddress.ip_address(s)
        return True
    except ValueError:
        return False

def extract_whoIs_features(urlsHostnames):
    
    total_urls = len(urlsHostnames)
    console.log("Loading urlsHostnames ...")

    try:
        with open("url_whois_features.pickle", "rb") as f:
            urlWhoIsFeaturesDict = pickle.load(f)
    except FileNotFoundError:
        urlWhoIsFeaturesDict = {}

    counter = 0

    def process_url(url):
        nonlocal counter
        whois_info = None

        if url not in urlWhoIsFeaturesDict:

            try:
                whois_info = whois.whois(url)
            except whois.parser.PywhoisError:
                whois_info = "NOT-REGISTRED"
            except re.error:
                print(f"Regex error with domain: {url}")
                whois_info = "REGEX-ERROR"
            except Exception as e:
                print(f"Unhandled exception in process_url for {url}: {e}")
                whois_info = "EXCEPTION"
        
        counter += 1
        progress_bar.update(1)

        return url, whois_info

    with concurrent.futures.ThreadPoolExecutor() as executor:
        progress_bar = tqdm(total=total_urls, desc="WHOIS-FEATURE-EXTRACTION")
        futures = [executor.submit(process_url, url) for url in urlsHostnames]
        
        for future in concurrent.futures.as_completed(futures):
            url, whois_info = future.result()
            
            if whois_info is not None:
                urlWhoIsFeaturesDict[url] = whois_info

            if counter % 5250 == 0:
                os.makedirs('whois', exist_ok=True)
                with open("whois/url_whois_features"+str(counter)+".pickle", "wb") as f:
                    pickle.dump(urlWhoIsFeaturesDict, f)

            if counter % 500 == 0:
                with open("url_whois_features.pickle", "wb") as f:
                    pickle.dump(urlWhoIsFeaturesDict, f)
    
    with open("url_whois_features.pickle", "wb") as f:
        pickle.dump(urlWhoIsFeaturesDict, f)

    return urlWhoIsFeaturesDict

def update_whoIs_features(urlsHostnames):
    
    total_urls = len(urlsHostnames)
    console.log("Loading urlsHostnames ...")


    urlWhoIsFeaturesDict = {}

    counter = 0

    def process_url(url):
        nonlocal counter
        whois_info = None

        if url not in urlWhoIsFeaturesDict:

            try:
                whois_info = whois.whois(url)
            except whois.parser.PywhoisError:
                whois_info = "NOT-REGISTRED"
            except re.error:
                print(f"Regex error with domain: {url}")
                whois_info = "REGEX-ERROR"
            except Exception as e:
                print(f"Unhandled exception in process_url for {url}: {e}")
                whois_info = "EXCEPTION"
        
        counter += 1
        progress_bar.update(1)

        return url, whois_info

    with concurrent.futures.ThreadPoolExecutor() as executor:
        progress_bar = tqdm(total=total_urls, desc="WHOIS-FEATURE-EXTRACTION")
        futures = [executor.submit(process_url, url) for url in urlsHostnames]
        
        for future in concurrent.futures.as_completed(futures):
            url, whois_info = future.result()
            
            if whois_info is not None:
                urlWhoIsFeaturesDict[url] = whois_info

    return urlWhoIsFeaturesDict

def categorize_whois_data(whois_data):
    category_counts = {"NOT-REGISTRED": 0, "REGEX-ERROR": 0, "EXCEPTION": 0, "NULL": 0, "VALID": 0}
    categorized_data = {"NOT-REGISTRED": set(), "REGEX-ERROR": set(), "EXCEPTION": set(), "NULL": set(), "VALID": {}}

    for url, whois_info in whois_data.items():
        if whois_info in ["NOT-REGISTRED", "REGEX-ERROR", "EXCEPTION"]:
            category_counts[whois_info] += 1
            categorized_data[whois_info].add(url)
        elif isinstance(whois_info, dict) and all(val is None for val in whois_info.values()):
            category_counts["NULL"] += 1
            categorized_data["NULL"].add(url)
        else:
            category_counts["VALID"] += 1
            categorized_data["VALID"][url] = whois_info

    return category_counts, categorized_data

def print_category_counts(category_counts):
    for category, count in category_counts.items():
        print(f"{category.lower()}: {count}")

with open("urls_hostnames.pickle", "rb") as f:
    urls_hostnames = pickle.load(f)

whoIsFeatures = extract_whoIs_features(urls_hostnames)

valid_count_diff = 1  # Initialize with a non-zero value for the first iteration

while valid_count_diff > 0:
    # Categorize whois data and print category counts
    category_counts, categorized_data = categorize_whois_data(whoIsFeatures)
    print_category_counts(category_counts)

    valid_count_diff = 0
     
    # Update urls_hostnames with unresolved urls and update valid_count
    unresolved_urls_hostnames = categorized_data['NULL'].union(categorized_data['NOT-REGISTRED'])
    newWhoIsFeatures = extract_whoIs_features(unresolved_urls_hostnames)
    
    whoIsFeatures.update(newWhoIsFeatures)
    category_counts_new, _ = categorize_whois_data(whoIsFeatures)

    valid_count_diff = category_counts_new["VALID"] - category_counts["VALID"]  # Update the valid_count
    console.log("Added " + str(valid_count_diff) + " valid urls ...")

    # Save the unresolved urls and the final overall_whois_features to files
    with open("unresolved_whois_urls.pickle", "wb") as f:
        pickle.dump(unresolved_urls_hostnames, f)

    with open("url_whois_features.pickle", "wb") as f:
        pickle.dump(whoIsFeatures, f)