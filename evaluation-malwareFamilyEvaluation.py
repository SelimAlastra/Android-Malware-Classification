import json
import pickle
import numpy as np
from rich.console import Console
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix
import pickle
from sklearn.svm import LinearSVC
from urllib.parse import urlparse
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import ipaddress
import math
from collections import Counter
from sklearn.feature_extraction import DictVectorizer
from tqdm import tqdm
from datetime import datetime
#Only use date from datetime for loading data
import random
import matplotlib.pyplot as plt


console = Console()

with open("urls_hostnames.pickle", "rb") as f:
    urls_hostnames = pickle.load(f)

urls_hostnames.discard(None)

with open("url_shodan_features.pickle", "rb") as f:
    shodanFeatures = pickle.load(f)

with open("url_whois_features.pickle", "rb") as f:
    whoIsFeatures = pickle.load(f)

def load_dataset(dataset_path, use_cache=True):

    # if use_cache:
    #     console.log('Trying to load dataset from cache')
    #     try:
    #         with open('cache.pkl', 'rb') as f:
    #             console.log('Loaded dataset from cache')
    #             return pickle.load(f)
    #     except FileNotFoundError:
    #         pass

    console.log(f'Loading dataset from {dataset_path}')

    with open('{}-X-updated.json'.format(dataset_path), 'r') as f:
        X = json.load(f)

    console.log('Loading labels...')
    with open('{}-y-updated.json'.format(dataset_path), 'rt') as f:
        y = json.load(f)

    console.log('Loading timestamps...')
    with open('{}-meta-updated.json'.format(dataset_path), 'rt') as f:
        T = json.load(f)
    T = [o['dex_date'] for o in T]
    T = [datetime.datetime.strptime(o, '%Y-%m-%dT%H:%M:%S') if "T" in o
             else datetime.datetime.strptime(o, '%Y-%m-%d %H:%M:%S') for o in T]

    console.log('Loading extended features...')
    with open('{}-meta-updated.json'.format(dataset_path), 'rt') as f:
        extended_features = json.load(f)

    time_index = {}
    for i in range(len(T)):
        t = T[i]
        if t.year not in time_index:
            time_index[t.year] = {}
        if t.month not in time_index[t.year]:
            time_index[t.year][t.month] = []
        time_index[t.year][t.month].append(i)
    
    data = X, y, time_index, T, extended_features
    # with open('cache.pkl', 'wb') as f:
    #     pickle.dump(data, f)
    
    return data

def replace_underscores(X):
    new_X = []
    for d in X:
        new_d = {}
        for k, v in d.items():
            new_k = k.replace('_', '.')
            new_d[new_k] = v
        new_X.append(new_d)
    return new_X

def dict_to_2d_list(X_dict):

    console.log('Converting into X into 2D list ...')

    X_list = []
    for x_dict in X_dict:
        x_list = list(x_dict.keys())
        X_list.append(x_list)
    return X_list

def filterData(X, y, filter_key):

    console.log(f"Filtering data with filter_key: ({filter_key})....")
    filtered_X = []
    filtered_y = []
    prefix_len = len(filter_key)

    for i in range(len(X)):
        if any(k.startswith(filter_key) for k in X[i]):
            filtered_dict = {k[prefix_len:]: v for k, v in X[i].items() if k.startswith(filter_key)}
            filtered_X.append(filtered_dict)
            filtered_y.append(y[i])

    filtered_X = replace_underscores(filtered_X)

    return filtered_X, filtered_y

def subsample_program(X, y, fraction):
    
    console.log("Subsampling data ...")

    split_index = int(len(X) * fraction)

    X_filtered = X[:split_index]
    y_filtered = y[:split_index]

    return X_filtered, y_filtered

def evaluate_classifier(clf, X_test, y_test):
    """
    Evaluates a trained classifier on a given testing set.

    Args:
        clf: The trained classifier.
        X_test: The testing set features.
        y_test: The testing set labels.

    Returns:
        A dictionary of the performance metrics including testing accuracy, false positive rate,
        false negative rate, F1 score, and detection rate.
    """

    print('Evaluating Classifier...')

    # Test the classifier on the testing set
    y_pred = clf.predict(X_test)
    accuracy = clf.score(X_test, y_test)
    f1score = f1_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    false_positive_rate = fp / (fp + tn)
    false_negative_rate = fn / (fn + tp)
    detection_rate = tp / (tp + fn)

    print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score, detection_rate)

    results = {
        "accuracy": accuracy,
        "false_positive_rate": false_positive_rate,
        "false_negative_rate": false_negative_rate,
        "f1_score": f1score,
        "detection_rate": detection_rate
    }

    return accuracy, false_positive_rate, false_negative_rate, f1score, detection_rate

def evaluate_voting(predictions, y_test):
    """
    Evaluates the classifier based on the provided predictions and ground truth labels.
    
    Parameters:
    - predictions: List of predictions
    - y_test: Ground truth labels
    
    Returns:
    - Prints the evaluation metrics
    """
    
    # Calculate F1 score
    f1 = f1_score(y_test, predictions)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, predictions)

    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

    # Calculate false positive rate and false negative rate
    false_positive_rate = fp / (fp + tn)
    false_negative_rate = fn / (fn + tp)
    detection_rate = tp / (tp + fn)

    print("EVALUATING VOTING CLASSIFIER ....")
    # Print the evaluation metrics
    print("F1 score:", f1)
    print("Accuracy:", accuracy)
    print("False Positive Rate:", false_positive_rate)
    print("False Negative Rate:", false_negative_rate)
    print("Detection Rate:", detection_rate)

    return f1, accuracy, false_positive_rate, false_negative_rate, detection_rate

def print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score, detection_rate):
    """
    Prints the evaluation metrics of a classifier.

    Args:
        accuracy (float): The testing accuracy of the classifier.
        false_positive_rate (float): The false positive rate of the classifier.
        false_negative_rate (float): The false negative rate of the classifier.
        f1score (float): The F1 score of the classifier.
    """
    print(f'Testing accuracy: {accuracy}')
    print(f'False Positive rate: {false_positive_rate}')
    print(f'False Negative rate: {false_negative_rate}')
    print(f'F1 Score: {f1score}')
    print(f'Detection Rate: {detection_rate}')

def is_ip_address(url):
    try:
        ipaddress.ip_address(url)
        return True
    except ValueError:
        return False
    
def create_features_df(X_2D):

    high_risk_tlds = [".sbs", ".cyou", ".quest", ".autos", ".top", ".makeup", ".skin", ".cloud", ".best", ".icu", ".cfd", ".shop", ".homes", ".xyz", ".bio", ".monster", ".ink", ".life", ".pics", ".bd", ".pro", ".py", ".live", ".th", ".run", ".ltd", ".store", ".today", ".id", ".buzz", ".club", ".space", ".website", ".pk", ".tz", ".am", ".site", ".click", ".mk", ".cc", ".ng", ".beauty", ".link", ".cn", ".lk", ".pe", ".tr", ".su", ".ke", ".asia"]
    #From netcraft: https://trends.netcraft.com/cybercrime/tlds

    console.log("Extracting features ...")

    def calculate_number_of_urls(urls):
        if len(urls) == 0:
            return 0
        return len(urls)
    
    def calculate_average_length(urls):
        if len(urls) == 0:
            return 0
        return sum(len(url) for url in urls) / len(urls)
    
    def calculate_letters_numbers_ratio(urls):
        if len(urls) == 0:
            return 0
        total_ratio = 0
        count = 0

        for url in urls:
            letters = sum(char.isalpha() for char in url)
            numbers = sum(char.isdigit() for char in url)

            if letters + numbers > 0:
                ratio = letters / (letters + numbers)
                total_ratio += ratio
                count += 1

        average_ratio = total_ratio / count if count != 0 else 0
        return average_ratio
    
    def parse_urls(X_2D):
        
        parsed_urls = {}

        for urls in X_2D:
            for url in urls:
                if url not in parsed_urls:
                    try:
                        parsed_urls[url] = urlparse(url)
                    except ValueError:
                        parsed_urls[url] = None

        return parsed_urls
    
    parsed_urls = parse_urls(X_2D)
    
    def calculateRatioOfValidUrls(urls):
        if len(urls) == 0:
            return 0
        total = len(urls)
        validTotal = sum(parsed_urls[url] is not None for url in urls)
        ratio = validTotal / total if total != 0 else 0
        return ratio
    
    def calculate_average_domain_length(urls):
        if len(urls) == 0:
            return 0
        valid_urls = [url for url in urls if parsed_urls[url]]
        total = sum(len(parsed_urls[url].netloc) for url in valid_urls)
        average = total / len(valid_urls) if valid_urls else 0
        return average

    def calculate_average_path_length(urls):
        if len(urls) == 0:
            return 0
        valid_urls = [url for url in urls if parsed_urls[url]]
        total = sum(len(parsed_urls[url].path) for url in valid_urls)
        average = total / len(valid_urls) if valid_urls else 0
        return average
    
    def calculate_average_subdomains(urls):
        if len(urls) == 0:
            return 0
        valid_urls = [url for url in urls if parsed_urls[url]]
        total_subdomains = 0

        for url in valid_urls:
            netloc = parsed_urls[url].netloc
            parts = netloc.split('.')
            # Subtract 2 to exclude main domain and TLD. If there's www or any other prefix, it will be counted.
            num_subdomains = max(0, len(parts) - 2)
            total_subdomains += num_subdomains

        average_subdomains = total_subdomains / len(valid_urls) if valid_urls else 0
        return average_subdomains

    def calculate_high_risk_tld_count(urls):
        if len(urls) == 0:
            return 0
        valid_urls = [url for url in urls if parsed_urls[url]]
        high_risk_tld_count = 0

        for url in valid_urls:
            netloc = parsed_urls[url].netloc
            tld = '.' + netloc.split('.')[-1]  # get the last part after the last dot, which should be the TLD
            if tld in high_risk_tlds:
                high_risk_tld_count += 1

        return high_risk_tld_count
    
    def calculate_avg_count_for_keyword(urls, keyword):
        if len(urls) == 0:
            return 0
        valid_urls = [url for url in urls if parsed_urls[url]]
        keyword_counts = [url.count(keyword) for url in valid_urls]

        average = sum(keyword_counts) / len(valid_urls) if valid_urls else 0

        return average

    def calculate_avg_login_count(urls):
        if len(urls) == 0:
            return 0
        return calculate_avg_count_for_keyword(urls, 'login')

    def calculate_avg_account_count(urls):
        if len(urls) == 0:
            return 0
        return calculate_avg_count_for_keyword(urls, 'account')

    def calculate_avg_verify_count(urls):
        if len(urls) == 0:
            return 0
        return calculate_avg_count_for_keyword(urls, 'verify')

    def calculate_avg_client_count(urls):
        if len(urls) == 0:
            return 0
        return calculate_avg_count_for_keyword(urls, 'client')

    def calculate_avg_admin_count(urls):
        if len(urls) == 0:
            return 0
        return calculate_avg_count_for_keyword(urls, 'admin')

    def calculate_avg_server_count(urls):
        if len(urls) == 0:
            return 0
        return calculate_avg_count_for_keyword(urls, 'server')


    def calculate_number_of_ip_addresses(urls):
        if len(urls) == 0:
            return 0
        total = sum(is_ip_address(parsed_urls[url].netloc) for url in urls if parsed_urls[url])
        return total

    def calculate_https_ratio(urls):
        if len(urls) == 0:
            return 0
        valid_urls = [url for url in urls if parsed_urls[url]]
        https = sum(parsed_urls[url] and parsed_urls[url].scheme == "https" if parsed_urls[url] is not None else False for url in urls)
        ratio = https / len(valid_urls) if valid_urls != 0 else 0
        return ratio
    
    # Define a function to calculate the number of dots and its average
    def calculate_avg_num_dots(urls):
        if len(urls) == 0:
            return 0
        num_dots = sum(url.count('.') for url in urls)
        avg_num_dots = num_dots / len(urls)
        return avg_num_dots

    # Define a function to calculate the number of directories and its average
    def calculate_avg_num_dirs(urls):
        if len(urls) == 0:
            return 0
        num_dirs = sum(url.count('/') for url in urls)
        avg_num_dirs = num_dirs - (2*len(urls)) / len(urls)
        #We remove the two initals slashes // from the scheme from the count
        return avg_num_dirs
    
    # Define a function to calculate the number of "#" occurrences and its average
    def calculate_avg_num_fragments(urls):
        if len(urls) == 0:
            return 0
        num_fragmnets = sum(url.count('#') for url in urls)
        avg_num_fragments = num_fragmnets / len(urls)
        return avg_num_fragments

    # Define a function to calculate the number of "@" occurrences and its average
    def calculate_avg_num_at(urls):
        if len(urls) == 0:
            return 0
        num_ats = sum(url.count('@') for url in urls)
        avg_num_ats = num_ats / len(urls)
        return avg_num_ats

    # Define a function to calculate the number of "%" [Encoded] occurrences and its average
    def calculate_avg_num_encoded(urls):
        if len(urls) == 0:
            return 0
        num_encoded = sum(url.count('%') for url in urls)
        avg_num_encoded = num_encoded / len(urls)
        return avg_num_encoded
    
    # Define a function to calculate the number of queries occurrences and its average
    def calculate_avg_num_queries(urls):
        if len(urls) == 0:
            return 0
        num_queries = sum(url.count('?') for url in urls)
        avg_num_queries = num_queries / len(urls)
        return avg_num_queries


    # Define a function to calculate the number of "=" occurrences and its average
    def calculate_avg_num_equal(urls):
        num_equals = sum(url.count('=') for url in urls)
        avg_num_equals = num_equals / len(urls)
        return avg_num_equals
    
    def calculate_avg_entropy(urls):
        if len(urls) == 0:
            return 0
        
        total = 0

        for url in urls:
            # Count the frequency of each symbol in the string
            symbol_counts = Counter(url)

            # Calculate the probabilities of each symbol
            total_symbols = len(url)
            probabilities = [count / total_symbols for count in symbol_counts.values()]

            # Calculate the entropy using the formula
            entropy = -sum(p * math.log2(p) for p in probabilities)

            total += entropy
        
        return total / len(urls)
    
    def count_url_shortened(urls):
        if len(urls) == 0:
            return 0

        shortened_services = ['bit.ly', 'goo.gl', 't.co', 'ow.ly', 'tinyurl', 'is.gd', 'v.gd', 'clck.ru', 'rebrand.ly', 'soo.gd', 'cutt.ly', 'u.nu', 'fur.ly', 'adcrun.ch', 'shorl.com', 'x.co', 'bc.vc', 'adf.ly', 'ouo.io', 'tiny.cc', 'clk.im', 'tr.im', 'short.am', 'rb.gy', 'shorte.st', 'shorturl.at', 'lnkd.in', 'viid.me', 'adfly', 'qr.net', 'bit.do', 'zz.gd', 'adcraft.co', 'moourl.com', 'vzturl.com', 'met.bz', 'filoops.info', 'ity.im', 'short.cm', 'linkshrink.net', 'shorten.to', 'link.tl']
        count = 0

        for url in urls:

            parsed_url = parsed_urls[url]
        
            if parsed_url:
                domain = parsed_url.netloc.lower()
                if domain in shortened_services:
                    count += 1

        return count

    def count_number_of_ports(urls):
        if len(urls) == 0:
            return 0
        count = 0

        for url in urls:
            parsed_url = parsed_urls[url]
            try: 
                if parsed_url and parsed_url.port is not None:
                        port = int(parsed_url.port)
                        count += 1
            except ValueError:
                continue

        return count
    
    # Define a function to calculate the number of digits and its average
    def calculate_avg_num_digits(urls):
        if len(urls) == 0:
            return 0
        num_digits = sum(url.count(char) for url in urls for char in '0123456789')
        avg_num_digits = num_digits / len(urls)
        return avg_num_digits
    
    def calculate_avg_num_letters(urls):
        if len(urls) == 0:
            return 0
        num_letters = sum(url.count(char) for url in urls for char in 'abcdefghiklmnopqrstuvwxyz')
        avg_num_letters = num_letters / len(urls)
        return avg_num_letters
    
    # Create a DataFrame with the URLs
    df = pd.DataFrame({'URLs': X_2D})

    df["Number of URLs"] = df['URLs'].apply(calculate_number_of_urls)
    console.log("Number of URLs calculation completed.")

    df["Number of IPs addresses"] = df['URLs'].apply(calculate_number_of_ip_addresses)
    console.log("Number of IP addresses calculation completed.")

    df["Average Length of URLs"] = df['URLs'].apply(calculate_average_length)
    console.log("Average Length of URLs calculation completed.")

    df["Ratio Valid URLS"] = df['URLs'].apply(calculateRatioOfValidUrls)
    console.log("Ratio Valid URLs calculation completed.")

    df["Average Domains Length"] = df['URLs'].apply(calculate_average_domain_length)
    console.log("Average Domains Length calculation completed.")

    df["Average Paths Length"] = df['URLs'].apply(calculate_average_path_length)
    console.log("Average Paths Length calculation completed.")

    df["Average Subdomain Count"] = df['URLs'].apply(calculate_average_subdomains)
    console.log("Average Subdomain Count calculation completed.")

    df["High Risk TLD Count"] = df['URLs'].apply(calculate_high_risk_tld_count)
    console.log("High Risk TLD Count calculation completed.")

    df["Average Login Count"] = df['URLs'].apply(calculate_avg_login_count)
    console.log("Average Login Count calculation completed.")

    df["Average Account Count"] = df['URLs'].apply(calculate_avg_account_count)
    console.log("Average Account Count calculation completed.")

    df["Average Verify Count"] = df['URLs'].apply(calculate_avg_verify_count)
    console.log("Average Verify Count calculation completed.")

    df["Average Client Count"] = df['URLs'].apply(calculate_avg_client_count)
    console.log("Average Client Count calculation completed.")

    df["Average Admin Count"] = df['URLs'].apply(calculate_avg_admin_count)
    console.log("Average Admin Count calculation completed.")

    df["Average Server Count"] = df['URLs'].apply(calculate_avg_server_count)
    console.log("Average Server Count calculation completed.")

    df["HTTPS ratio"] = df['URLs'].apply(calculate_https_ratio)
    console.log("HTTPS ratio calculation completed.")

    df["Average Number of Dots"] = df['URLs'].apply(calculate_avg_num_dots)
    console.log("Average Number of Dots calculation completed.")

    df["Average Number of Paths"] = df['URLs'].apply(calculate_avg_num_dirs)
    console.log("Average Number of Directories calculation completed.")

    df["Average Number of At Symbols"] = df['URLs'].apply(calculate_avg_num_at)
    console.log("Average Number of At Symbols calculation completed.")

    df["Average Number of Fragments"] = df['URLs'].apply(calculate_avg_num_fragments)
    console.log("Average Number of fragments calculation completed.")

    df["Average Number of Encoded Characters"] = df['URLs'].apply(calculate_avg_num_encoded)
    console.log("Average Number of encoded characters calculation completed.")

    df["Average Number of Queries"] = df['URLs'].apply(calculate_avg_num_queries)
    console.log("Average Number of queries calculation completed.")

    df["Average Entropy"] = df['URLs'].apply(calculate_avg_entropy)
    console.log("Average Entropy calculation completed.")

    df["Count of Shortened Services"] = df['URLs'].apply(count_url_shortened)
    console.log("Number of shortened services used calculated.")

    df["Number of Ports"] = df['URLs'].apply(count_number_of_ports)
    console.log("Number of ports used calculated.")

    df["Number of Digits"] = df['URLs'].apply(calculate_avg_num_digits)
    console.log("Avg Number of digits used calculated.")

    df["Number of Letters"] = df['URLs'].apply(calculate_avg_num_letters)
    console.log("Avg Number of letters used calculated.")

    df["Letters/Numbers Ratio Average"] = df['URLs'].apply(calculate_letters_numbers_ratio)
    console.log("Letters/Numbers Ratio calculation completed.")

    #NOT USED from here

    # df["Average Number of Hyphens"] = df['URLs'].apply(calculate_avg_num_hyphen)
    # console.log("Average Number of Hyphens calculation completed.")

    # df["Average Number of Dollar Signs"] = df['URLs'].apply(calculate_avg_num_dollar)
    # console.log("Average Number of Dollar Signs calculation completed.")

    # df["Average Number of Ampersands"] = df['URLs'].apply(calculate_avg_num_ampersand)
    # console.log("Average Number of Ampersands calculation completed.")

    # df["Average Number of Equal Signs"] = df['URLs'].apply(calculate_avg_num_equal)
    # console.log("Average Number of Equal Signs calculation completed.")

    # df["Total Length of URLs"] = df['URLs'].apply(calculate_total_length_of_urls)
    # console.log("Total Length of URLs calculation completed.")

    # df['Average TLD length'] = df['URLs'].apply(calculate_avg_tld_length)
    # console.log("Average TLD length calculated.")

    # df["Total Paths Length"] = df['URLs'].apply(calculate_total_path_length)
    # console.log("Total Paths Length calculation completed.")

    # df["Total Domains Length"] = df['URLs'].apply(calculate_total_domain_length)
    # console.log("Total Domains Length calculation completed.")

    # df["Total Length of URLs"] = df['URLs'].apply(calculate_total_length_of_urls)
    # console.log("Total Length of URLs calculation completed.")

    # Drop the 'URLs' and 'Average Length' columns from the DataFrame
    df = df.drop(['URLs'], axis=1)

    return df

def tokenize_urls(X_2D):

    console.log("Tokenising URLs")

    X_features = []

    for urls in X_2D:

        features = {}

        for url in urls:

            try: 

                parsed_url = urlparse(url)

                # Scheme token
                if parsed_url.scheme != "":
                    if "scheme::"+parsed_url.scheme in features:
                        features["scheme::"+parsed_url.scheme] += 1
                    else:
                        features["scheme::"+parsed_url.scheme] = 1

                # Netloc token  
                if parsed_url.netloc != "":
                    if "netloc::"+parsed_url.netloc in features:
                        features["netloc::"+parsed_url.netloc] += 1
                    else:
                        features["netloc::"+parsed_url.netloc] = 1

                # Path token
                if parsed_url.path != "":
                    if "path::"+parsed_url.path in features:
                        features["path::"+parsed_url.path] += 1
                    else:
                        features["path::"+parsed_url.path] = 1

                # Query token
                if parsed_url.query != "":
                    if "query::"+parsed_url.query in features:
                        features["query::"+parsed_url.query] += 1
                    else:
                        features["query::"+parsed_url.query] = 1
                    
                # Fragment token
                if parsed_url.fragment != "":
                    if "fragment::"+parsed_url.fragment in features:
                        features["fragment::"+parsed_url.fragment] += 1
                    else:
                        features["fragment::"+parsed_url.fragment] = 1

                # Username token
                if parsed_url.username is not None:
                    if "username::"+parsed_url.username in features:
                        features["username::"+parsed_url.username] += 1
                    else:
                        features["username::"+parsed_url.username] = 1

                # Password token
                if parsed_url.password is not None:
                    if "password::"+parsed_url.password in features:
                        features["password::"+parsed_url.password] += 1
                    else:
                        features["password::"+parsed_url.password] = 1

                # Hostname token
                if parsed_url.hostname is not None:
                    if "hostname::"+parsed_url.hostname in features:
                        features["hostname::"+parsed_url.hostname] += 1
                    else:
                        features["hostname::"+parsed_url.hostname] = 1

                # Port token
                if parsed_url.port is not None:
                    if "port::"+str(parsed_url.port) in features:
                        features["port::"+str(parsed_url.port)] += 1
                    else:
                        features["port::"+str(parsed_url.port)] = 1

            except ValueError as e:

                continue

        X_features.append(features)

    return X_features

def getUrlShodanFeaturesDict(shodanFeatures, url):

    features = {}
    position_info = {}

    errCodes = ["API-ERROR","FAILED-TO-GET-IP","UNICODE-ERROR", "EXCEPTION"] 

    keys = ['region_code', 'tags', 'ip', 'area_code', 'domains', 
            'hostnames', 'country_code', 'org', 'data', 'asn', 'city', 
            'latitude', 'isp', 'longitude', 'last_update', 'country_name', 
            'ip_str', 'os', 'ports', 'vulns']

    shodanInfo = shodanFeatures[url]

    for errCode in errCodes:
        if shodanInfo == errCode:
            return {'shodan-'+errCode:1}, None
        
    for key in keys:
        value = shodanInfo.get(key)
        if value is not None:
            if key == 'latitude':
                position_info['latitude'] = value
            elif key == 'longitude':
                position_info['longitude'] = value
            if isinstance(value, list):
                for element in value:
                    features[key + "::" + str(element)] = 1
            else:
                features[key + "-shodan-::" + str(value)] = 1
        else: features[key + "-shodan-::NotFound"] = 1

    # features.update(position_info)

    return features, position_info

def getUrlWhoIsFeaturesDict(whoIsFeatures, url):

    features = {}
    date_info = {}

    whoIsInfo = whoIsFeatures[url]
    # print(whoIsInfo)

    errCodes = ["NOT-REGISTRED","REGEX-ERROR","EXCEPTION"] 
    
    for errCode in errCodes:
        if whoIsInfo == errCode:
            return {'whois'+errCode:1}, None

    keys = ["domain_name", "registrar", "referral_url", "dnssec", "name", 
            "org", "address", "city", "state", "registrant_postal_code", 
            "country", "name_servers", "emails", "registrar_country",
            ]
    
    for key in keys:
        value = whoIsInfo.get(key)
        if value is not None:
            if isinstance(value, list):
                for element in value:
                    features[key + "::" + element] = 1
            else:
                features[key + "-whois-::" + value] = 1
        else: features[key + "-whois-::NotFound"] = 1

    date_keys = ["creation_date", "updated_date", "expiration_date", "last_modified"]
    nowDate = datetime.now().timestamp()

    for date_key in date_keys:
        value = whoIsInfo.get(date_key)
        if value is not None:
            if isinstance(value, list):
                try:
                    date_info[date_key] = value[0].timestamp()
                except Exception as e:
                    # print(e)
                    if len(value) > 1: date_info[date_key] = value[1].timestamp()
                    
            elif isinstance(value, str):
                try:
                    # Use split to divide the string into two parts at "(GMT"
                    # We will only use the first part, which we get by accessing [0]
                    value = value.split("(GMT")[0].strip()

                    # Now value is "2004-07-23 02:18:42 "
                    # Parse it with datetime.strptime
                    date_format = "%Y-%m-%d %H:%M:%S"
                    date_object = datetime.strptime(value, date_format)
                    date_info[date_key] = date_object.timestamp()
                except Exception as e:
                    continue
            else:
                date_info[date_key] = value.timestamp()

    if "creation_date" in date_info and "expiration_date" in date_info:
        date_info["whois-lifespan"] = date_info["expiration_date"] - date_info["creation_date"]
    
    if "creation_date" in date_info:
        date_info["whois-age"] = nowDate - date_info["creation_date"]
    
    if "expiration_date" in date_info:
        date_info["whois-lifeRemaining"] = date_info["expiration_date"] - nowDate
    
    if "last_modified" in date_info:
        date_info["whois-last_modified"] = date_info["last_modified"]

    # features.update(date_info)

    return features, date_info

def getUrlsHostFeatures(shodanFeatures, whoIsFeatures, urls):

    features = {}
    numerical_info = []
 
    for url in urls:

        shodan_features, position_info = getUrlShodanFeaturesDict(shodanFeatures, url)
        whois_features, date_info = getUrlWhoIsFeaturesDict(whoIsFeatures, url)

        numerical_info.append(position_info)
        numerical_info.append(date_info)

        for key, value in shodan_features.items():
            if key in features:
                features[key] += value
            else:
                features[key] = value

        for key, value in whois_features.items():
            if key in features:
                features[key] += value
            else:
                features[key] = value

    numericalValuesKeys = ["creation_date", "updated_date", "expiration_date",
                            "last_modified", "latitude", "longitude", "whois-lifespan",
                            "whois-age", "whois-lifeRemaining", "whois-last_modified"]

    for key in numericalValuesKeys:
        values = [d.get(key) for d in numerical_info if d is not None and key in d]
        count = len(values)
        if count > 0:
            average = sum(values) / count
            features[key] = average

    return features

def vectorize_features(X_features):

    console.log("Vectorizing ...")

    # Create an instance of DictVectorizer
    vectorizer = DictVectorizer()

    X_vectorized = vectorizer.fit_transform(X_features)

    feature_names = vectorizer.get_feature_names_out()

    return X_vectorized, feature_names, vectorizer

def vectorize_features_voting(X_features, vectoriser):

    console.log("Vectorizing ...")

    X_vectorized = vectoriser.transform(X_features)

    feature_names = vectoriser.get_feature_names_out()

    return X_vectorized, feature_names

def train_random_forest(vectorized_X, y_vectorized, feature_names, n_jobs=-1):

    console.log('Training Random Forest Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorized_X, y_vectorized, test_size=0.1, random_state=21)

    # Train the Random Forest classifier
    clf = RandomForestClassifier(n_jobs=n_jobs, random_state=21)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    # evaluate_classifier(clf, X_test, y_test)

    # console.log('Finding most important features..')

    # feature_importances = clf.feature_importances_
    # top_indices = np.argsort(feature_importances)[::-1][:10]
    # top_features = [feature_names[i] for i in top_indices]

    # print(f"Top 10 features: {top_features}")
    # print(f"Feature importances: {feature_importances[top_indices]}")

    return clf, X_train, X_test, y_train, y_test

def unknown_train_random_forest(vectorized_X, y_vectorized, feature_names, n_jobs=-1):
    console.log('Training Random Forest Classifier...')

    # Train the Random Forest classifier
    clf = RandomForestClassifier(n_jobs=n_jobs, random_state=21)
    clf.fit(vectorized_X, y_vectorized)

    # Test the classifier on the testing set
    # evaluate_classifier(clf, X_test, y_test)

    # console.log('Finding most important features..')

    # feature_importances = clf.feature_importances_
    # top_indices = np.argsort(feature_importances)[::-1][:10]
    # top_features = [feature_names[i] for i in top_indices]

    # print(f"Top 10 features: {top_features}")
    # print(f"Feature importances: {feature_importances[top_indices]}")

    return clf

def get_family_name(labels):

    """Helper function for getting family names"""

    for label in labels:

        if label.startswith('FAM'):

            family = label.split('FAM:')[1].split('|')[0]

            return family

# Find all malware families in a given year and month #
def vt_label_split(dataset_path):
    frame = pd.read_csv(dataset_path, delimiter='\t')
    frame = frame.loc[~frame['families'].isnull() & ~(frame['families'] == '[]')]
    malware_time_split = {}
    family_names = {}
    for fam, date, md5 in zip(frame["families"], frame["dex_date"], frame["md5"]):
        labels = fam.split(',')
        if get_family_name(labels):
            family = get_family_name(labels)
            if type(date) is str:
                if "t" not in date:
                    date = datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')
                else:
                    date = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S')
                year = date.year
                month = date.month
                if year not in malware_time_split:
                    malware_time_split[year] = {}
                if month not in malware_time_split[year]:
                    malware_time_split[year][month] = {}
                if family not in malware_time_split[year][month]:
                    malware_time_split[year][month][family] = []
                if family not in family_names:
                    family_names[family] = 0
                family_names[family] += 1
                malware_time_split[year][month][family].append(md5.upper())

    return malware_time_split

def create_subset(X, y, familyIndexes, family_name):
    """
    Create a subset of data with samples from a specific malware family and 8 times benign samples.
    
    Parameters:
    - X: The feature matrix
    - y: The labels
    - familyIndexes: Dictionary mapping families to their indexes in the dataset
    - family_name: The name of the malware family to extract
    
    Returns:
    - subsetX: Subset of the feature matrix
    - subsetY: Subset of the labels
    """
    
    # Extract malicious samples
    maliciousX = []
    maliciousY = []

    for maliciousIndex in familyIndexes[family_name]:
        maliciousX.append(X[maliciousIndex])
        maliciousY.append(y[maliciousIndex])

    # Identify benign samples
    benign_indexes = [i for i, label in enumerate(y) if label == 0]

    # Randomly select 8 times benign samples for every malicious sample
    number_of_malicious_samples = len(maliciousX)
    number_of_benign_samples_needed = 8 * number_of_malicious_samples

    if number_of_benign_samples_needed > len(benign_indexes):
        print("Not enough benign samples!")
        # You can decide how to handle this, maybe by using all available benign samples
        selected_benign_indexes = benign_indexes
    else:
        selected_benign_indexes = random.sample(benign_indexes, number_of_benign_samples_needed)

    benignX = [X[i] for i in selected_benign_indexes]
    benignY = [y[i] for i in selected_benign_indexes]

    # Combine malicious and benign samples
    subsetX = maliciousX + benignX
    subsetY = maliciousY + benignY

    return subsetX, subsetY

def create_unknown_family_subset(X, y, familyIndexes, family_name):
    
    # Exclude samples from the family being evaluated
    unknownIndexes = familyIndexes[family_name]

    unknownX = [X[i] for i in unknownIndexes[:100]]
    unknownY = [y[i] for i in unknownIndexes[:100]]

    # Identify benign samples
    benign_indexes = [i for i, label in enumerate(y) if label == 0]

    # Identify other malicious samples that are not from the family being evaluated
    other_malicious_indexes = [i for i, label in enumerate(y) if label == 1 and i not in unknownIndexes]

    # Randomly select 400 malicious samples
    if len(other_malicious_indexes) < 400:
        print("Not enough other malicious samples!")
        selected_malicious_indexes = other_malicious_indexes
    else:
        selected_malicious_indexes = random.sample(other_malicious_indexes, 400)

    maliciousX = [X[i] for i in selected_malicious_indexes]
    maliciousY = [y[i] for i in selected_malicious_indexes]

    # Randomly select 8 times benign samples for every malicious sample
    number_of_benign_samples_needed = 8 * (len(maliciousX) + 100)

    if number_of_benign_samples_needed > len(benign_indexes):
        print("Not enough benign samples!")
        selected_benign_indexes = benign_indexes
    else:
        selected_benign_indexes = random.sample(benign_indexes, number_of_benign_samples_needed)

    benignX = [X[i] for i in selected_benign_indexes]
    benignY = [y[i] for i in selected_benign_indexes]

    # Combine malicious and benign samples
    subsetX = maliciousX + benignX
    subsetY = maliciousY + benignY

    return subsetX, unknownX, subsetY, unknownY

def create_unknown_family_subset_with_samples(X, y, familyIndexes, family_name):    
    # Exclude samples from the family being evaluated
    unknown_indexes = familyIndexes[family_name]

    providedSamplesX = [X[i] for i in unknown_indexes[:5]]
    providedSamplesY = [y[i] for i in unknown_indexes[:5]]

    unknownTestX =  [X[i] for i in unknown_indexes[:-100]]
    unknownTestY =  [y[i] for i in unknown_indexes[:-100]]

    # Identify benign samples
    benign_indexes = [i for i, label in enumerate(y) if label == 0]

    # Identify other malicious samples that are not from the family being evaluated
    other_malicious_indexes = [i for i, label in enumerate(y) if label == 1 and i not in unknown_indexes]

    # Randomly select 400 malicious samples
    if len(other_malicious_indexes) < 400:
        print("Not enough other malicious samples!")
        selected_malicious_indexes = other_malicious_indexes
    else:
        selected_malicious_indexes = random.sample(other_malicious_indexes, 400)

    maliciousX = [X[i] for i in selected_malicious_indexes]
    maliciousY = [y[i] for i in selected_malicious_indexes]

    # Randomly select 8 times benign samples for every malicious sample
    number_of_benign_samples_needed = 8 * (len(maliciousX) + 105)

    if number_of_benign_samples_needed > len(benign_indexes):
        print("Not enough benign samples!")
        selected_benign_indexes = benign_indexes
    else:
        selected_benign_indexes = random.sample(benign_indexes, number_of_benign_samples_needed)

    benignX = [X[i] for i in selected_benign_indexes]
    benignY = [y[i] for i in selected_benign_indexes]

    # Combine malicious, benign, and excluded samples
    subsetX_train = maliciousX + benignX + providedSamplesX
    subsetY_train = maliciousY + benignY + providedSamplesY

    return subsetX_train, unknownTestX, subsetY_train, unknownTestY

with open("saved_data.pkl", "rb") as file:
    X, y, time_index, T, extended_features = pickle.load(file)

with open("familyIndexes.pkl", 'rb') as file:
    familyIndexes = pickle.load(file)

def getMalwareFamilyResults(subsetX, subsetY):
    
    X_filtered, y_filtered = filterData(subsetX, subsetY, "urls::")

    X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.1, random_state=21)

    X_2D = dict_to_2d_list(replace_underscores(X_filtered))

    #####LEXICAL#####

    console.log("LEXICAL")

    X_lexical_df = create_features_df(X_2D)

    lexical_feature_names = X_lexical_df.columns.tolist()

    lexical_rf_clf, X_lexical_train, X_lexical_test, y_lexical_train, y_lexical_test = train_random_forest(X_lexical_df, y_filtered, lexical_feature_names)

    lexical_accuracy, lexical_false_positive_rate, lexical_false_negative_rate, lexical_f1score, lexical_detection_rate = evaluate_classifier(lexical_rf_clf, X_lexical_test, y_lexical_test)

    ####TOKENS####

    console.log("TOKENS")

    X_tokens_features = tokenize_urls(X_2D)

    X_tokens_vectorized, tokens_feature_names, tokens_vectoriser = vectorize_features(X_tokens_features)

    tokens_rf_clf, X_tokens_train, X_tokens_test, y_tokens_train, y_tokens_test = train_random_forest(X_tokens_vectorized, y_filtered, tokens_feature_names)

    tokens_accuracy, tokens_false_positive_rate, tokens_false_negative_rate, tokens_f1score, tokens_detection_rate = evaluate_classifier(tokens_rf_clf, X_tokens_test, y_tokens_test)

    ####HOSTBASED####

    console.log("HOST")

    X_host_features = []

    with tqdm(total=len(X_2D), desc="Processing URLs") as pbar:
        
        for urls in X_2D:
            collected_urls_hostnames = set()
            for url in urls:
                try:
                    url_hostname = urlparse(url).hostname
                    if url_hostname in urls_hostnames: collected_urls_hostnames.add(url_hostname)
                except Exception:
                    continue
            
            hostBasedInfo = getUrlsHostFeatures(shodanFeatures, whoIsFeatures, collected_urls_hostnames)
            X_host_features.append(hostBasedInfo)
            pbar.update(1)

    X_host_vectorized, host_feature_names, host_vectoriser = vectorize_features(X_host_features)

    host_rf_clf, X_host_train, X_host_test, y_host_train, y_host_test = train_random_forest(X_host_vectorized, y_filtered, host_feature_names)
    host_accuracy, host_false_positive_rate, host_false_negative_rate, host_f1score, host_detection_rate = evaluate_classifier(host_rf_clf, X_host_test, y_host_test)

    #####VOTING#####
    # For Lexical Features
    X_test_lexical_features = create_features_df(X_test)

    # For Token Features
    X_test_tokens_features = tokenize_urls(X_test)
    X_test_tokens_vectorized, X_test_tokens_feature_names = vectorize_features_voting(X_test_tokens_features, tokens_vectoriser)

    # For Host-Based Features
    X_test_host_features = []

    with tqdm(total=len(X_test), desc="Extracting HostBased Features") as pbar:
        for urls in X_test:
            collected_urls_hostnames = set()
            for url in urls:
                try:
                    url_hostname = urlparse(url).hostname
                    if url_hostname in urls_hostnames: collected_urls_hostnames.add(url_hostname)
                except Exception:
                    continue
            
            hostBasedInfo = getUrlsHostFeatures(shodanFeatures, whoIsFeatures, collected_urls_hostnames)
            X_test_host_features.append(hostBasedInfo)
            pbar.update(1)

    X_test_host_vectorized, X_test_host_feature_names = vectorize_features_voting(X_test_host_features, host_vectoriser)

    # Predict probabilities for each feature type
    prob_pred_lexical = lexical_rf_clf.predict_proba(X_test_lexical_features)
    prob_pred_tokens = tokens_rf_clf.predict_proba(X_test_tokens_vectorized)
    prob_pred_host = host_rf_clf.predict_proba(X_test_host_vectorized)

    ###Hard Voting
    combined_predictions = []
    console.log("Hard Voting")
    for p1, p2, p3 in zip(prob_pred_lexical, prob_pred_tokens, prob_pred_host):

        votes_for_malware = int(p1[1] > 0.5) + int(p2[1] > 0.5) + int(p3[1] > 0.5)

        # Majority vote: If 2 or more classifiers predict malware (1), then the combined prediction is malware.
        if votes_for_malware > 1:
            combined_predictions.append(1)
        else:
            combined_predictions.append(0)

    return evaluate_voting(combined_predictions, y_test)

def getMalwareFamilyResultsUnkownFamily(subsetX_train, subsetX_test, subsetY_train, subsetY_test):
    
    X_filtered, y_filtered = filterData(subsetX_train, subsetY_train, "urls::")
    subsetX_test, subsetY_test = filterData(subsetX_test, subsetY_test, "urls::")

    X_2D = dict_to_2d_list(replace_underscores(X_filtered))

    #####LEXICAL#####

    console.log("LEXICAL")

    X_lexical_df = create_features_df(X_2D)

    lexical_feature_names = X_lexical_df.columns.tolist()

    lexical_rf_clf = unknown_train_random_forest(X_lexical_df, y_filtered, lexical_feature_names)

    ####TOKENS####

    console.log("TOKENS")

    X_tokens_features = tokenize_urls(X_2D)

    X_tokens_vectorized, tokens_feature_names, tokens_vectoriser = vectorize_features(X_tokens_features)

    tokens_rf_clf = unknown_train_random_forest(X_tokens_vectorized, y_filtered, tokens_feature_names)

    ####HOSTBASED####

    console.log("HOST")

    X_host_features = []

    with tqdm(total=len(X_2D), desc="Processing URLs") as pbar:
        
        for urls in X_2D:
            collected_urls_hostnames = set()
            for url in urls:
                try:
                    url_hostname = urlparse(url).hostname
                    if url_hostname in urls_hostnames: collected_urls_hostnames.add(url_hostname)
                except Exception:
                    continue
            
            hostBasedInfo = getUrlsHostFeatures(shodanFeatures, whoIsFeatures, collected_urls_hostnames)
            X_host_features.append(hostBasedInfo)
            pbar.update(1)

    X_host_vectorized, host_feature_names, host_vectoriser = vectorize_features(X_host_features)

    host_rf_clf = unknown_train_random_forest(X_host_vectorized, y_filtered, host_feature_names)

    #####VOTING#####
    # For Lexical Features
    X_test_lexical_features = create_features_df(subsetX_test)

    # For Token Features
    X_test_tokens_features = tokenize_urls(subsetX_test)
    X_test_tokens_vectorized, X_test_tokens_feature_names = vectorize_features_voting(X_test_tokens_features, tokens_vectoriser)

    # For Host-Based Features
    X_test_host_features = []

    with tqdm(total=len(subsetX_test), desc="Extracting HostBased Features") as pbar:
        for urls in subsetX_test:
            collected_urls_hostnames = set()
            for url in urls:
                try:
                    url_hostname = urlparse(url).hostname
                    if url_hostname in urls_hostnames: collected_urls_hostnames.add(url_hostname)
                except Exception:
                    continue
            
            hostBasedInfo = getUrlsHostFeatures(shodanFeatures, whoIsFeatures, collected_urls_hostnames)
            X_test_host_features.append(hostBasedInfo)
            pbar.update(1)

    X_test_host_vectorized, X_test_host_feature_names = vectorize_features_voting(X_test_host_features, host_vectoriser)

    # Predict probabilities for each feature type
    prob_pred_lexical = lexical_rf_clf.predict_proba(X_test_lexical_features)
    prob_pred_tokens = tokens_rf_clf.predict_proba(X_test_tokens_vectorized)
    prob_pred_host = host_rf_clf.predict_proba(X_test_host_vectorized)

    ###Hard Voting
    combined_predictions = []
    console.log("Hard Voting")
    for p1, p2, p3 in zip(prob_pred_lexical, prob_pred_tokens, prob_pred_host):

        votes_for_malware = int(p1[1] > 0.5) + int(p2[1] > 0.5) + int(p3[1] > 0.5)

        # Majority vote: If 2 or more classifiers predict malware (1), then the combined prediction is malware.
        if votes_for_malware > 1:
            combined_predictions.append(1)
        else:
            combined_predictions.append(0)

    return evaluate_voting(combined_predictions, subsetY_test)

family_names = ["kuguo", "dnotua", "dowgin", "revmob", "airpush", "smsreg", "inmobi", "feiwo", "leadbolt", "ewind"]

# ####Detection of unknown malware families#####
# Slice the list to get the desired families for testing
test_families = family_names # You can modify this line to select specific families

# Lists to store the results
NoSamplesKnown = []
TwentySamplesKnown = []

# Loop through each family name in test_families and get the results
for family in tqdm(test_families, desc="Processing families"):
    # For known families
    subsetX_train, subsetX_test, subsetY_train, subsetY_test = create_unknown_family_subset(X, y, familyIndexes, family)
    f1, accuracy, false_positive_rate, false_negative_rate, detection_rate = getMalwareFamilyResultsUnkownFamily(subsetX_train, subsetX_test, subsetY_train, subsetY_test)
    NoSamplesKnown.append(detection_rate)
    
    # For unknown families
    subsetX_train, subsetX_test, subsetY_train, subsetY_test = create_unknown_family_subset_with_samples(X, y, familyIndexes, family)
    f1, accuracy, false_positive_rate, false_negative_rate, detection_rate = getMalwareFamilyResultsUnkownFamily(subsetX_train, subsetX_test, subsetY_train, subsetY_test)
    TwentySamplesKnown.append(detection_rate)

print("\n")

# Calculate and print the average, lowest, and highest detection rates for NoSamplesKnown
print("NoSamplesKnown Results:")
print(f"Average Detection Rate: {sum(NoSamplesKnown) / len(NoSamplesKnown):.2f}")
print(f"Lowest Detection Rate: {min(NoSamplesKnown):.2f}")
print(f"Highest Detection Rate: {max(NoSamplesKnown):.2f}")

# Calculate and print the average, lowest, and highest detection rates for TwentySamplesKnown
print("\nTwentySamplesKnown Results:")
print(f"Average Detection Rate: {sum(TwentySamplesKnown) / len(TwentySamplesKnown):.2f}")
print(f"Lowest Detection Rate: {min(TwentySamplesKnown):.2f}")
print(f"Highest Detection Rate: {max(TwentySamplesKnown):.2f}")

###GRAPH###
# Setting up the positions for bars
barWidth = 0.35
r1 = np.arange(len(NoSamplesKnown))
r2 = [x + barWidth for x in r1]

# Plotting
plt.bar(r1, NoSamplesKnown, color='#6B8E23', width=barWidth, edgecolor='grey', label='Zero Samples Available')
plt.bar(r2, TwentySamplesKnown, color='#FFA500', width=barWidth, edgecolor='grey', label='Five Samples Available')

# Adding labels
plt.xlabel('Family Names', fontweight='bold')
# Adjust the xticks to be centered between the two bars
plt.xticks([r + barWidth/2 for r in range(len(NoSamplesKnown))], test_families, rotation=45, ha='center')  # Adjusted line

plt.ylabel('Detection Rate')
plt.legend()

plt.title('Detection of unknown families')
plt.tight_layout()
plt.show()

# ####Detection of malware families#####
# Slice the list to get the last two family names
# test_families = family_names

# # Lists to store the results
# f1_scores = []
# detection_rates = []

# # Loop through each family name in test_families and get the results
# for family in test_families:
#     subsetX, subsetY = create_subset(X, y, familyIndexes, family)
#     f1, accuracy, false_positive_rate, false_negative_rate, detection_rate = getMalwareFamilyResults(subsetX, subsetY)
#     f1_scores.append(f1)
#     detection_rates.append(detection_rate)

# # Calculate average, highest, and lowest for F1 scores
# avg_f1 = sum(f1_scores) / len(f1_scores)
# max_f1 = max(f1_scores)
# min_f1 = min(f1_scores)

# # Calculate average, highest, and lowest for detection rates
# avg_detection_rate = sum(detection_rates) / len(detection_rates)
# max_detection_rate = max(detection_rates)
# min_detection_rate = min(detection_rates)

# # Print the results
# print("\n")
# print(f"Average F1 Score: {avg_f1:.2f}")
# print(f"Highest F1 Score: {max_f1:.2f}")
# print(f"Lowest F1 Score: {min_f1:.2f}")
# print("\n")
# print(f"Average Detection Rate: {avg_detection_rate:.2f}")
# print(f"Highest Detection Rate: {max_detection_rate:.2f}")
# print(f"Lowest Detection Rate: {min_detection_rate:.2f}")

# ###GRAPH###
# # Setting up the positions for bars
# barWidth = 0.35
# r1 = np.arange(len(f1_scores))
# r2 = [x + barWidth for x in r1]

# # Plotting
# plt.bar(r1, f1_scores, color='red', width=barWidth, edgecolor='grey', label='F1 Score')
# plt.bar(r2, detection_rates, color='blue', width=barWidth, edgecolor='grey', label='Detection Rate')

# # Adding labels
# plt.xlabel('Family Names', fontweight='bold')
# # Adjust the xticks to be centered between the two bars
# plt.xticks([r + barWidth/2 for r in range(len(f1_scores))], test_families, rotation=45, ha='center')  # Adjusted line

# plt.ylabel('Score')
# plt.legend()

# plt.title('F1 Score and Detection Rate for Test Malware Families')
# plt.tight_layout()
# plt.show()

#####OTHER USEFUL CODE#####

# # Sort the dictionary based on the length of the lists in descending order
# sorted_families_indexes = sorted(familyIndexes.items(), key=lambda item: len(item[1]), reverse=True)

# # Extract and print the top 5 families and their corresponding lists
# for family, samples in sorted_families_indexes[:5]:
#     print(f"Family: {family}, Number of indexes: {len(samples)}")

# total_samples = len(y)
# malicious_samples = sum(y)  # This works if malicious is labeled as 1 and benign as 0

# malicious_ratio = malicious_samples / total_samples

# print(f"Ratio of malicious apps: {malicious_ratio:.2f}")
# Ratio of malicious apps: 0.11
# Approximatelly have 8 times the number of benign apps to achieve the same distribution as in the original dataset

#### CODE TO GET FAMILY INDEXES ####
# malware_time_split = vt_label_split("dataset/meta_info_file.tsv")

# familyDict = {}

# for a in range(2014,2018):
#     for b in range(1,12):
#         for family in malware_time_split[a][b]:
#             if family not in familyDict:
#                 familyDict[family] = malware_time_split[a][b][family]
#             else: 
#                 familyDict[family].extend(malware_time_split[a][b][family])

# # Sort the dictionary based on the length of the lists in descending order
# sorted_families = sorted(familyDict.items(), key=lambda item: len(item[1]), reverse=True)

# # # Extract and print the top 5 families and their corresponding lists
# # for family, samples in sorted_families[:5]:
# #     print(f"Family: {family}, Number of MD5 Hashes: {len(samples)}")

# # Initialize counters
# total_malicious_apps = 0
# apps_found_in_families = 0

# # Iterate over all instances in y
# for i in range(len(y)):
#     if y[i] == 1:  # If the instance is malicious
#         total_malicious_apps += 1
#         md5_hash_to_check = extended_features[i]['md5']

#         # Check if the md5 hash is in any family
#         for family, md5_hashes in familyDict.items():
#             if md5_hash_to_check in md5_hashes:
#                 apps_found_in_families += 1
#                 break

# # # Calculate the ratio
# # if total_malicious_apps > 0:
# #     ratio = apps_found_in_families / total_malicious_apps
# #     print(f"Ratio of malicious apps found in families: {ratio:.2f}")
# # else:
# #     print("No malicious apps found.")

# def extract_malware_family_indexes(X, extended_features, familyDict):

#     familyIndexes = {}

#     for i in range(len(X)):
#         for family in familyDict:
#             if extended_features[i]['md5'] in familyDict[family]:
#                 # Check if the family key exists in familyIndexes
#                 if family not in familyIndexes:
#                     familyIndexes[family] = []
#                 familyIndexes[family].append(i)
#     return familyIndexes

# familyIndexes = extract_malware_family_indexes(X, extended_features, familyDict)

# # Save the dictionary to a pickle file
# with open("familyIndexes.pkl", 'wb') as file:
#     pickle.dump(familyIndexes, file)

# print(f"familyIndexes saved to familyIndexes.pkl")

# # Sort the dictionary based on the length of the lists in descending order
# sorted_families_indexes = sorted(familyIndexes.items(), key=lambda item: len(item[1]), reverse=True)

# # Extract and print the top 5 families and their corresponding lists
# for family, samples in sorted_families_indexes[:5]:
#     print(f"Family: {family}, Number of indexes: {len(samples)}")

# print(familyIndexes["airpush"])