import json
import pickle
import datetime
import numpy as np
from rich.console import Console
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix
import pickle
from sklearn.svm import LinearSVC
from urllib.parse import urlparse
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import ipaddress
import math
from collections import Counter
import tldextract
from sklearn.feature_extraction import DictVectorizer
from tqdm import tqdm
from datetime import datetime

console = Console()

console.log("Running Voting System ...")

with open("urls_hostnames.pickle", "rb") as f:
    urls_hostnames = pickle.load(f)

urls_hostnames.discard(None)

with open("url_shodan_features.pickle", "rb") as f:
    shodanFeatures = pickle.load(f)

with open("url_whois_features.pickle", "rb") as f:
    whoIsFeatures = pickle.load(f)

def load_dataset(dataset_path, use_cache=True):

    if use_cache:
        console.log('Trying to load dataset from cache')
        try:
            with open('cache.pkl', 'rb') as f:
                console.log('Loaded dataset from cache')
                return pickle.load(f)
        except FileNotFoundError:
            pass

    console.log(f'Loading dataset from {dataset_path}')

    with open('{}-X-updated.json'.format(dataset_path), 'r') as f:
        X = json.load(f)
    # if not shas:
    #     [o.pop('sha256') for o in X]

    console.log('Loading labels...')
    with open('{}-y-updated.json'.format(dataset_path), 'rt') as f:
        y = json.load(f)
    # if 'apg' not in dataset_path:
    #     y = [o[0] for o in y]

    console.log('Loading timestamps...')
    with open('{}-meta-updated.json'.format(dataset_path), 'rt') as f:
        T = json.load(f)
    T = [o['dex_date'] for o in T]
    T = [datetime.datetime.strptime(o, '%Y-%m-%dT%H:%M:%S') if "T" in o
             else datetime.datetime.strptime(o, '%Y-%m-%d %H:%M:%S') for o in T]

    time_index = {}
    for i in range(len(T)):
        t = T[i]
        if t.year not in time_index:
            time_index[t.year] = {}
        if t.month not in time_index[t.year]:
            time_index[t.year][t.month] = []
        time_index[t.year][t.month].append(i)
    
    data = X, y, time_index, T
    with open('cache.pkl', 'wb') as f:
        pickle.dump(data, f)

    return data

def replace_underscores(X):
    new_X = []
    for d in X:
        new_d = {}
        for k, v in d.items():
            new_k = k.replace('_', '.')
            new_d[new_k] = v
        new_X.append(new_d)
    return new_X

def dict_to_2d_list(X_dict):

    console.log('Converting into X into 2D list ...')

    X_list = []
    for x_dict in X_dict:
        x_list = list(x_dict.keys())
        X_list.append(x_list)
    return X_list

def filterData(X, y, filter_key):

    console.log(f"Filtering data with filter_key: ({filter_key})....")
    filtered_X = []
    filtered_y = []
    prefix_len = len(filter_key)

    for i in range(len(X)):
        if any(k.startswith(filter_key) for k in X[i]):
            filtered_dict = {k[prefix_len:]: v for k, v in X[i].items() if k.startswith(filter_key)}
            filtered_X.append(filtered_dict)
            filtered_y.append(y[i])

    filtered_X = replace_underscores(filtered_X)

    return filtered_X, filtered_y

def subsample_program(X, y, fraction):
    
    console.log("Subsampling data ...")

    split_index = int(len(X) * fraction)

    X_filtered = X[:split_index]
    y_filtered = y[:split_index]

    return X_filtered, y_filtered

def evaluate_classifier(clf, X_test, y_test):
    """
    Evaluates a trained classifier on a given testing set.

    Args:
        clf: The trained classifier.
        X_test: The testing set features.
        y_test: The testing set labels.

    Returns:
        A dictionary of the performance metrics including testing accuracy, false positive rate,
        false negative rate, and F1 score.
    """

    console.log('Evaluating Classifier...')

    # Test the classifier on the testing set
    y_pred = clf.predict(X_test)
    accuracy = clf.score(X_test, y_test)
    f1score = f1_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    false_positive_rate = fp / (fp + tn)
    false_negative_rate = fn / (fn + tp)

    print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score)

    results = {
        "accuracy": accuracy,
        "false_positive_rate": false_positive_rate,
        "false_negative_rate": false_negative_rate,
        "f1_score": f1score,
    }

    return results

def evaluate_voting(predictions, y_test):
    """
    Evaluates the classifier based on the provided predictions and ground truth labels.
    
    Parameters:
    - predictions: List of predictions
    - y_test: Ground truth labels
    
    Returns:
    - Prints the evaluation metrics
    """
    
    # Calculate F1 score
    f1 = f1_score(y_test, predictions)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, predictions)

    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

    # Calculate false positive rate and false negative rate
    false_positive_rate = fp / (fp + tn)
    false_negative_rate = fn / (fn + tp)

    print("EVALUATING VOTING CLASSIFIER ....")
    # Print the evaluation metrics
    print("F1 score:", f1)
    print("Accuracy:", accuracy)
    print("False Positive Rate:", false_positive_rate)
    print("False Negative Rate:", false_negative_rate)

def print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score):
    """
    Prints the evaluation metrics of a classifier.

    Args:
        accuracy (float): The testing accuracy of the classifier.
        false_positive_rate (float): The false positive rate of the classifier.
        false_negative_rate (float): The false negative rate of the classifier.
        f1score (float): The F1 score of the classifier.
    """
    print(f'Testing accuracy: {accuracy}')
    print(f'False Positive rate: {false_positive_rate}')
    print(f'False Negative rate: {false_negative_rate}')
    print(f'F1 Score: {f1score}')

def is_ip_address(url):
    try:
        ipaddress.ip_address(url)
        return True
    except ValueError:
        return False
    
def create_features_df(X_2D):

    high_risk_tlds = [".sbs", ".cyou", ".quest", ".autos", ".top", ".makeup", ".skin", ".cloud", ".best", ".icu", ".cfd", ".shop", ".homes", ".xyz", ".bio", ".monster", ".ink", ".life", ".pics", ".bd", ".pro", ".py", ".live", ".th", ".run", ".ltd", ".store", ".today", ".id", ".buzz", ".club", ".space", ".website", ".pk", ".tz", ".am", ".site", ".click", ".mk", ".cc", ".ng", ".beauty", ".link", ".cn", ".lk", ".pe", ".tr", ".su", ".ke", ".asia"]
    #From netcraft: https://trends.netcraft.com/cybercrime/tlds

    console.log("Extracting features ...")

    def calculate_number_of_urls(urls):
        return len(urls)
    
    def calculate_average_length(urls):
        return sum(len(url) for url in urls) / len(urls)
    
    def calculate_letters_numbers_ratio(urls):
        total_ratio = 0
        count = 0

        for url in urls:
            letters = sum(char.isalpha() for char in url)
            numbers = sum(char.isdigit() for char in url)

            if letters + numbers > 0:
                ratio = letters / (letters + numbers)
                total_ratio += ratio
                count += 1

        average_ratio = total_ratio / count if count != 0 else 0
        return average_ratio
    
    def parse_urls(X_2D):
        
        parsed_urls = {}

        for urls in X_2D:
            for url in urls:
                if url not in parsed_urls:
                    try:
                        parsed_urls[url] = urlparse(url)
                    except ValueError:
                        parsed_urls[url] = None

        return parsed_urls
    
    parsed_urls = parse_urls(X_2D)
    
    def calculateRatioOfValidUrls(urls):
        total = len(urls)
        validTotal = sum(parsed_urls[url] is not None for url in urls)
        ratio = validTotal / total if total != 0 else 0
        return ratio
    
    def calculate_average_domain_length(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        total = sum(len(parsed_urls[url].netloc) for url in valid_urls)
        average = total / len(valid_urls) if valid_urls else 0
        return average

    def calculate_average_path_length(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        total = sum(len(parsed_urls[url].path) for url in valid_urls)
        average = total / len(valid_urls) if valid_urls else 0
        return average
    
    def calculate_average_subdomains(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        total_subdomains = 0

        for url in valid_urls:
            netloc = parsed_urls[url].netloc
            parts = netloc.split('.')
            # Subtract 2 to exclude main domain and TLD. If there's www or any other prefix, it will be counted.
            num_subdomains = max(0, len(parts) - 2)
            total_subdomains += num_subdomains

        average_subdomains = total_subdomains / len(valid_urls) if valid_urls else 0
        return average_subdomains

    def calculate_high_risk_tld_count(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        high_risk_tld_count = 0

        for url in valid_urls:
            netloc = parsed_urls[url].netloc
            tld = '.' + netloc.split('.')[-1]  # get the last part after the last dot, which should be the TLD
            if tld in high_risk_tlds:
                high_risk_tld_count += 1

        return high_risk_tld_count
    
    def calculate_avg_count_for_keyword(urls, keyword):
        valid_urls = [url for url in urls if parsed_urls[url]]
        keyword_counts = [url.count(keyword) for url in valid_urls]

        average = sum(keyword_counts) / len(valid_urls) if valid_urls else 0

        return average

    def calculate_avg_login_count(urls):
        return calculate_avg_count_for_keyword(urls, 'login')

    def calculate_avg_account_count(urls):
        return calculate_avg_count_for_keyword(urls, 'account')

    def calculate_avg_verify_count(urls):
        return calculate_avg_count_for_keyword(urls, 'verify')

    def calculate_avg_client_count(urls):
        return calculate_avg_count_for_keyword(urls, 'client')

    def calculate_avg_admin_count(urls):
        return calculate_avg_count_for_keyword(urls, 'admin')

    def calculate_avg_server_count(urls):
        return calculate_avg_count_for_keyword(urls, 'server')


    def calculate_number_of_ip_addresses(urls):
        total = sum(is_ip_address(parsed_urls[url].netloc) for url in urls if parsed_urls[url])
        return total

    def calculate_https_ratio(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        https = sum(parsed_urls[url] and parsed_urls[url].scheme == "https" if parsed_urls[url] is not None else False for url in urls)
        ratio = https / len(valid_urls) if valid_urls != 0 else 0
        return ratio
    
    # Define a function to calculate the number of dots and its average
    def calculate_avg_num_dots(urls):
        num_dots = sum(url.count('.') for url in urls)
        avg_num_dots = num_dots / len(urls)
        return avg_num_dots

    # Define a function to calculate the number of directories and its average
    def calculate_avg_num_dirs(urls):
        num_dirs = sum(url.count('/') for url in urls)
        avg_num_dirs = num_dirs - (2*len(urls)) / len(urls)
        #We remove the two initals slashes // from the scheme from the count
        return avg_num_dirs
    
    # Define a function to calculate the number of "#" occurrences and its average
    def calculate_avg_num_fragments(urls):
        num_fragmnets = sum(url.count('#') for url in urls)
        avg_num_fragments = num_fragmnets / len(urls)
        return avg_num_fragments

    # Define a function to calculate the number of "@" occurrences and its average
    def calculate_avg_num_at(urls):
        num_ats = sum(url.count('@') for url in urls)
        avg_num_ats = num_ats / len(urls)
        return avg_num_ats

    # Define a function to calculate the number of "%" [Encoded] occurrences and its average
    def calculate_avg_num_encoded(urls):
        num_encoded = sum(url.count('%') for url in urls)
        avg_num_encoded = num_encoded / len(urls)
        return avg_num_encoded
    
    # Define a function to calculate the number of queries occurrences and its average
    def calculate_avg_num_queries(urls):
        num_queries = sum(url.count('?') for url in urls)
        avg_num_queries = num_queries / len(urls)
        return avg_num_queries


    # Define a function to calculate the number of "=" occurrences and its average
    def calculate_avg_num_equal(urls):
        num_equals = sum(url.count('=') for url in urls)
        avg_num_equals = num_equals / len(urls)
        return avg_num_equals
    
    def calculate_avg_entropy(urls):
        
        total = 0

        for url in urls:
            # Count the frequency of each symbol in the string
            symbol_counts = Counter(url)

            # Calculate the probabilities of each symbol
            total_symbols = len(url)
            probabilities = [count / total_symbols for count in symbol_counts.values()]

            # Calculate the entropy using the formula
            entropy = -sum(p * math.log2(p) for p in probabilities)

            total += entropy
        
        return total / len(urls)
    
    def count_url_shortened(urls):

        shortened_services = ['bit.ly', 'goo.gl', 't.co', 'ow.ly', 'tinyurl', 'is.gd', 'v.gd', 'clck.ru', 'rebrand.ly', 'soo.gd', 'cutt.ly', 'u.nu', 'fur.ly', 'adcrun.ch', 'shorl.com', 'x.co', 'bc.vc', 'adf.ly', 'ouo.io', 'tiny.cc', 'clk.im', 'tr.im', 'short.am', 'rb.gy', 'shorte.st', 'shorturl.at', 'lnkd.in', 'viid.me', 'adfly', 'qr.net', 'bit.do', 'zz.gd', 'adcraft.co', 'moourl.com', 'vzturl.com', 'met.bz', 'filoops.info', 'ity.im', 'short.cm', 'linkshrink.net', 'shorten.to', 'link.tl']
        count = 0

        for url in urls:

            parsed_url = parsed_urls[url]
        
            if parsed_url:
                domain = parsed_url.netloc.lower()
                if domain in shortened_services:
                    count += 1

        return count

    def count_number_of_ports(urls):
        
        count = 0

        for url in urls:
            parsed_url = parsed_urls[url]
            try: 
                if parsed_url and parsed_url.port is not None:
                        port = int(parsed_url.port)
                        count += 1
            except ValueError:
                continue

        return count
    
    # Define a function to calculate the number of digits and its average
    def calculate_avg_num_digits(urls):
        num_digits = sum(url.count(char) for url in urls for char in '0123456789')
        avg_num_digits = num_digits / len(urls)
        return avg_num_digits
    
    def calculate_avg_num_letters(urls):
        num_letters = sum(url.count(char) for url in urls for char in 'abcdefghiklmnopqrstuvwxyz')
        avg_num_letters = num_letters / len(urls)
        return avg_num_letters
    
            
    # Create a DataFrame with the URLs
    df = pd.DataFrame({'URLs': X_2D})

    df["Number of URLs"] = df['URLs'].apply(calculate_number_of_urls)
    console.log("Number of URLs calculation completed.")

    df["Number of IPs addresses"] = df['URLs'].apply(calculate_number_of_ip_addresses)
    console.log("Number of IP addresses calculation completed.")

    df["Average Length of URLs"] = df['URLs'].apply(calculate_average_length)
    console.log("Average Length of URLs calculation completed.")

    df["Ratio Valid URLS"] = df['URLs'].apply(calculateRatioOfValidUrls)
    console.log("Ratio Valid URLs calculation completed.")

    df["Average Domains Length"] = df['URLs'].apply(calculate_average_domain_length)
    console.log("Average Domains Length calculation completed.")

    df["Average Paths Length"] = df['URLs'].apply(calculate_average_path_length)
    console.log("Average Paths Length calculation completed.")

    df["Average Subdomain Count"] = df['URLs'].apply(calculate_average_subdomains)
    console.log("Average Subdomain Count calculation completed.")

    df["High Risk TLD Count"] = df['URLs'].apply(calculate_high_risk_tld_count)
    console.log("High Risk TLD Count calculation completed.")

    df["Average Login Count"] = df['URLs'].apply(calculate_avg_login_count)
    console.log("Average Login Count calculation completed.")

    df["Average Account Count"] = df['URLs'].apply(calculate_avg_account_count)
    console.log("Average Account Count calculation completed.")

    df["Average Verify Count"] = df['URLs'].apply(calculate_avg_verify_count)
    console.log("Average Verify Count calculation completed.")

    df["Average Client Count"] = df['URLs'].apply(calculate_avg_client_count)
    console.log("Average Client Count calculation completed.")

    df["Average Admin Count"] = df['URLs'].apply(calculate_avg_admin_count)
    console.log("Average Admin Count calculation completed.")

    df["Average Server Count"] = df['URLs'].apply(calculate_avg_server_count)
    console.log("Average Server Count calculation completed.")

    df["HTTPS ratio"] = df['URLs'].apply(calculate_https_ratio)
    console.log("HTTPS ratio calculation completed.")

    df["Average Number of Dots"] = df['URLs'].apply(calculate_avg_num_dots)
    console.log("Average Number of Dots calculation completed.")

    df["Average Number of Paths"] = df['URLs'].apply(calculate_avg_num_dirs)
    console.log("Average Number of Directories calculation completed.")

    df["Average Number of At Symbols"] = df['URLs'].apply(calculate_avg_num_at)
    console.log("Average Number of At Symbols calculation completed.")

    df["Average Number of Fragments"] = df['URLs'].apply(calculate_avg_num_fragments)
    console.log("Average Number of fragments calculation completed.")

    df["Average Number of Encoded Characters"] = df['URLs'].apply(calculate_avg_num_encoded)
    console.log("Average Number of encoded characters calculation completed.")

    df["Average Number of Queries"] = df['URLs'].apply(calculate_avg_num_queries)
    console.log("Average Number of queries calculation completed.")

    df["Average Entropy"] = df['URLs'].apply(calculate_avg_entropy)
    console.log("Average Entropy calculation completed.")

    df["Count of Shortened Services"] = df['URLs'].apply(count_url_shortened)
    console.log("Number of shortened services used calculated.")

    df["Number of Ports"] = df['URLs'].apply(count_number_of_ports)
    console.log("Number of ports used calculated.")

    df["Number of Digits"] = df['URLs'].apply(calculate_avg_num_digits)
    console.log("Avg Number of digits used calculated.")

    df["Number of Letters"] = df['URLs'].apply(calculate_avg_num_letters)
    console.log("Avg Number of letters used calculated.")

    df["Letters/Numbers Ratio Average"] = df['URLs'].apply(calculate_letters_numbers_ratio)
    console.log("Letters/Numbers Ratio calculation completed.")

    #NOT USED from here

    # df["Average Number of Hyphens"] = df['URLs'].apply(calculate_avg_num_hyphen)
    # console.log("Average Number of Hyphens calculation completed.")

    # df["Average Number of Dollar Signs"] = df['URLs'].apply(calculate_avg_num_dollar)
    # console.log("Average Number of Dollar Signs calculation completed.")

    # df["Average Number of Ampersands"] = df['URLs'].apply(calculate_avg_num_ampersand)
    # console.log("Average Number of Ampersands calculation completed.")

    # df["Average Number of Equal Signs"] = df['URLs'].apply(calculate_avg_num_equal)
    # console.log("Average Number of Equal Signs calculation completed.")

    # df["Total Length of URLs"] = df['URLs'].apply(calculate_total_length_of_urls)
    # console.log("Total Length of URLs calculation completed.")

    # df['Average TLD length'] = df['URLs'].apply(calculate_avg_tld_length)
    # console.log("Average TLD length calculated.")

    # df["Total Paths Length"] = df['URLs'].apply(calculate_total_path_length)
    # console.log("Total Paths Length calculation completed.")

    # df["Total Domains Length"] = df['URLs'].apply(calculate_total_domain_length)
    # console.log("Total Domains Length calculation completed.")

    # df["Total Length of URLs"] = df['URLs'].apply(calculate_total_length_of_urls)
    # console.log("Total Length of URLs calculation completed.")

    # Drop the 'URLs' and 'Average Length' columns from the DataFrame
    df = df.drop(['URLs'], axis=1)

    return df

def tokenize_urls(X_2D):

    console.log("Tokenising URLs")

    X_features = []

    for urls in X_2D:

        features = {}

        for url in urls:

            try: 

                parsed_url = urlparse(url)

                # Scheme token
                if parsed_url.scheme != "":
                    if "scheme::"+parsed_url.scheme in features:
                        features["scheme::"+parsed_url.scheme] += 1
                    else:
                        features["scheme::"+parsed_url.scheme] = 1

                # Netloc token  
                if parsed_url.netloc != "":
                    if "netloc::"+parsed_url.netloc in features:
                        features["netloc::"+parsed_url.netloc] += 1
                    else:
                        features["netloc::"+parsed_url.netloc] = 1

                # Path token
                if parsed_url.path != "":
                    if "path::"+parsed_url.path in features:
                        features["path::"+parsed_url.path] += 1
                    else:
                        features["path::"+parsed_url.path] = 1

                # Query token
                if parsed_url.query != "":
                    if "query::"+parsed_url.query in features:
                        features["query::"+parsed_url.query] += 1
                    else:
                        features["query::"+parsed_url.query] = 1
                    
                # Fragment token
                if parsed_url.fragment != "":
                    if "fragment::"+parsed_url.fragment in features:
                        features["fragment::"+parsed_url.fragment] += 1
                    else:
                        features["fragment::"+parsed_url.fragment] = 1

                # Username token
                if parsed_url.username is not None:
                    if "username::"+parsed_url.username in features:
                        features["username::"+parsed_url.username] += 1
                    else:
                        features["username::"+parsed_url.username] = 1

                # Password token
                if parsed_url.password is not None:
                    if "password::"+parsed_url.password in features:
                        features["password::"+parsed_url.password] += 1
                    else:
                        features["password::"+parsed_url.password] = 1

                # Hostname token
                if parsed_url.hostname is not None:
                    if "hostname::"+parsed_url.hostname in features:
                        features["hostname::"+parsed_url.hostname] += 1
                    else:
                        features["hostname::"+parsed_url.hostname] = 1

                # Port token
                if parsed_url.port is not None:
                    if "port::"+str(parsed_url.port) in features:
                        features["port::"+str(parsed_url.port)] += 1
                    else:
                        features["port::"+str(parsed_url.port)] = 1

            except ValueError as e:

                continue

        X_features.append(features)

    return X_features

def getUrlShodanFeaturesDict(shodanFeatures, url):

    features = {}
    position_info = {}

    errCodes = ["API-ERROR","FAILED-TO-GET-IP","UNICODE-ERROR", "EXCEPTION"] 

    keys = ['region_code', 'tags', 'ip', 'area_code', 'domains', 
            'hostnames', 'country_code', 'org', 'data', 'asn', 'city', 
            'latitude', 'isp', 'longitude', 'last_update', 'country_name', 
            'ip_str', 'os', 'ports', 'vulns']

    shodanInfo = shodanFeatures[url]

    for errCode in errCodes:
        if shodanInfo == errCode:
            return {'shodan-'+errCode:1}, None
        
    for key in keys:
        value = shodanInfo.get(key)
        if value is not None:
            if key == 'latitude':
                position_info['latitude'] = value
            elif key == 'longitude':
                position_info['longitude'] = value
            if isinstance(value, list):
                for element in value:
                    features[key + "::" + str(element)] = 1
            else:
                features[key + "-shodan-::" + str(value)] = 1
        else: features[key + "-shodan-::NotFound"] = 1

    # features.update(position_info)

    return features, position_info

def getUrlWhoIsFeaturesDict(whoIsFeatures, url):

    features = {}
    date_info = {}

    whoIsInfo = whoIsFeatures[url]
    # print(whoIsInfo)

    errCodes = ["NOT-REGISTRED","REGEX-ERROR","EXCEPTION"] 
    
    for errCode in errCodes:
        if whoIsInfo == errCode:
            return {'whois'+errCode:1}, None

    keys = ["domain_name", "registrar", "referral_url", "dnssec", "name", 
            "org", "address", "city", "state", "registrant_postal_code", 
            "country", "name_servers", "emails", "registrar_country",
            ]
    
    for key in keys:
        value = whoIsInfo.get(key)
        if value is not None:
            if isinstance(value, list):
                for element in value:
                    features[key + "::" + element] = 1
            else:
                features[key + "-whois-::" + value] = 1
        else: features[key + "-whois-::NotFound"] = 1

    date_keys = ["creation_date", "updated_date", "expiration_date", "last_modified"]
    nowDate = datetime.now().timestamp()

    for date_key in date_keys:
        value = whoIsInfo.get(date_key)
        if value is not None:
            if isinstance(value, list):
                try:
                    date_info[date_key] = value[0].timestamp()
                except Exception as e:
                    # print(e)
                    if len(value) > 1: date_info[date_key] = value[1].timestamp()
                    
            elif isinstance(value, str):
                try:
                    # Use split to divide the string into two parts at "(GMT"
                    # We will only use the first part, which we get by accessing [0]
                    value = value.split("(GMT")[0].strip()

                    # Now value is "2004-07-23 02:18:42 "
                    # Parse it with datetime.strptime
                    date_format = "%Y-%m-%d %H:%M:%S"
                    date_object = datetime.strptime(value, date_format)
                    date_info[date_key] = date_object.timestamp()
                except Exception as e:
                    continue
            else:
                date_info[date_key] = value.timestamp()

    if "creation_date" in date_info and "expiration_date" in date_info:
        date_info["whois-lifespan"] = date_info["expiration_date"] - date_info["creation_date"]
    
    if "creation_date" in date_info:
        date_info["whois-age"] = nowDate - date_info["creation_date"]
    
    if "expiration_date" in date_info:
        date_info["whois-lifeRemaining"] = date_info["expiration_date"] - nowDate
    
    if "last_modified" in date_info:
        date_info["whois-last_modified"] = date_info["last_modified"]

    # features.update(date_info)

    return features, date_info

def getUrlsHostFeatures(shodanFeatures, whoIsFeatures, urls):

    features = {}
    numerical_info = []
 
    for url in urls:

        shodan_features, position_info = getUrlShodanFeaturesDict(shodanFeatures, url)
        whois_features, date_info = getUrlWhoIsFeaturesDict(whoIsFeatures, url)

        numerical_info.append(position_info)
        numerical_info.append(date_info)

        for key, value in shodan_features.items():
            if key in features:
                features[key] += value
            else:
                features[key] = value

        for key, value in whois_features.items():
            if key in features:
                features[key] += value
            else:
                features[key] = value

    numericalValuesKeys = ["creation_date", "updated_date", "expiration_date",
                            "last_modified", "latitude", "longitude", "whois-lifespan",
                            "whois-age", "whois-lifeRemaining", "whois-last_modified"]

    for key in numericalValuesKeys:
        values = [d.get(key) for d in numerical_info if d is not None and key in d]
        count = len(values)
        if count > 0:
            average = sum(values) / count
            features[key] = average

    return features

def vectorize_features(X_features, vectoriser):

    console.log("Vectorizing ...")

    X_vectorized = vectoriser.transform(X_features)

    feature_names = vectoriser.get_feature_names_out()

    return X_vectorized, feature_names

with open("filtered_data.pickle", "rb") as f:
    console.log("Loaded filtered data from cache")
    X_filtered, y_filtered = pickle.load(f)

X_filtered, y_filtered = subsample_program(X_filtered, y_filtered, 1)

X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.1, random_state=21)

with open('lexical_f_rf_clf.pkl', 'rb') as fid:
    lexical_f_rf_clf = pickle.load(fid)

with open('tk2_f_random_forest_clf.pkl', 'rb') as fid:
    tk2_f_random_forest_clf = pickle.load(fid)

with open('tk2_x_vectoriser.pkl', 'rb') as fid:
    tk2_x_vectoriser = pickle.load(fid)

with open('hostBased_random_forest_clf.pkl', 'rb') as fid:
    host_random_forest_clf = pickle.load(fid)

with open('hostBased_random_forest_vectoriser.pkl', 'rb') as fid:
    host_x_vectoriser = pickle.load(fid)

X_test_lexical_features = create_features_df(X_test)

X_test_tk_features = tokenize_urls(X_test)
X_test_tk_features_vectorised, X_test_tk_features_names = vectorize_features(X_test_tk_features, tk2_x_vectoriser)

X_test_host_features = []
with tqdm(total=len(X_test), desc="Extracting HostBased Features") as pbar:
    
    for urls in X_test:
        collected_urls_hostnames = set()
        for url in urls:
            try:
                url_hostname = urlparse(url).hostname
                if url_hostname in urls_hostnames: collected_urls_hostnames.add(url_hostname)
            except Exception:
                continue
        
        hostBasedInfo = getUrlsHostFeatures(shodanFeatures, whoIsFeatures, collected_urls_hostnames)
        X_test_host_features.append(hostBasedInfo)
        pbar.update(1)
X_test_host_features_vectorised, X_test_host_features_names = vectorize_features(X_test_host_features, host_x_vectoriser)

prob_pred1 = lexical_f_rf_clf.predict_proba(X_test_lexical_features)
prob_pred2 = tk2_f_random_forest_clf.predict_proba(X_test_tk_features_vectorised)
prob_pred3 = host_random_forest_clf.predict_proba(X_test_host_features_vectorised)
# Returns list of arrays where the first element is the probability of having the class 0 and the second
# element is the probability of having the probability of the class 1 (malware)

# ###Hard Voting
# combined_predictions = []
# console.log("Hard Voting")
# for p1, p2, p3 in zip(prob_pred1, prob_pred2, prob_pred3):

#     votes_for_malware = int(p1[1] > 0.5) + int(p2[1] > 0.5) + int(p3[1] > 0.5)

#     # Majority vote: If 2 or more classifiers predict malware (1), then the combined prediction is malware.
#     if votes_for_malware > 1:
#         combined_predictions.append(1)
#     else:
#         combined_predictions.append(0)

# evaluate_voting(combined_predictions, y_test)

# ###Soft Voting
# combined_predictions = []
# console.log("Soft Voting")
# for p1, p2, p3 in zip(prob_pred1, prob_pred2, prob_pred3):
    
#     # Average probabilities of being malware (class 1) for the three classifiers
#     avg_malware_prob = (p1[1] + p2[1] + p3[1]) / 3

#     # If the average probability is greater than 0.5, classify as malware
#     if avg_malware_prob > 0.5:
#         combined_predictions.append(1)
#     else:
#         combined_predictions.append(0)

# evaluate_voting(combined_predictions, y_test)

# ###Weighted Voting 
# combined_predictions = []
# console.log("Weighted Voting")
# for p1, p2, p3 in zip(prob_pred1, prob_pred2, prob_pred3):
#     w1, w2, w3 = 0.2, 0.5, 0.3  # !!! These should sum to 1.
    
#     # Calculate the weighted average probabilities of being malware (class 1) for the three classifiers
#     weighted_avg_malware_prob = (w1 * p1[1] + w2 * p2[1] + w3 * p3[1])
    
#     # If the weighted average probability is greater than 0.5, classify as malware
#     if weighted_avg_malware_prob > 0.5:
#         combined_predictions.append(1)
#     else:
#         combined_predictions.append(0)

# evaluate_voting(combined_predictions, y_test)


### Hybrid Hard Soft -
# console.log("Hybrid Hard Soft")
# for p1, p2, p3 in zip(prob_pred1, prob_pred2, prob_pred3):

#     benignPredictionScore = p1[0] + p2[0] + p3[0]
#     malwarePredictionScore = p1[1] + p2[1] + p3[1]

#     if p1[1] > 0.75 or p2[1] > 0.75 or p3[1] > 0.75:  
#         combined_predictions.append(1)
#     elif p1[0] > 0.9 or p2[0] > 0.9 or p3[0] > 0.9:  
#         combined_predictions.append(0)
#     elif benignPredictionScore > malwarePredictionScore: 
#         combined_predictions.append(0)
#     else:
#         combined_predictions.append(1)





