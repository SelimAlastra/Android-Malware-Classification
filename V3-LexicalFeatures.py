import json
import pickle
import datetime
import numpy as np
from rich.console import Console
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pickle
from sklearn.svm import LinearSVC
from urllib.parse import urlparse
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import ipaddress
import math
from collections import Counter

console = Console()

console.log("Running Lexical Features ...")

def load_dataset(dataset_path, use_cache=True):

    if use_cache:
        console.log('Trying to load dataset from cache')
        try:
            with open('cache.pkl', 'rb') as f:
                console.log('Loaded dataset from cache')
                return pickle.load(f)
        except FileNotFoundError:
            pass

    console.log(f'Loading dataset from {dataset_path}')

    with open('{}-X-updated.json'.format(dataset_path), 'r') as f:
        X = json.load(f)
    # if not shas:
    #     [o.pop('sha256') for o in X]

    console.log('Loading labels...')
    with open('{}-y-updated.json'.format(dataset_path), 'rt') as f:
        y = json.load(f)
    # if 'apg' not in dataset_path:
    #     y = [o[0] for o in y]

    console.log('Loading timestamps...')
    with open('{}-meta-updated.json'.format(dataset_path), 'rt') as f:
        T = json.load(f)
    T = [o['dex_date'] for o in T]
    T = [datetime.datetime.strptime(o, '%Y-%m-%dT%H:%M:%S') if "T" in o
             else datetime.datetime.strptime(o, '%Y-%m-%d %H:%M:%S') for o in T]

    time_index = {}
    for i in range(len(T)):
        t = T[i]
        if t.year not in time_index:
            time_index[t.year] = {}
        if t.month not in time_index[t.year]:
            time_index[t.year][t.month] = []
        time_index[t.year][t.month].append(i)
    
    data = X, y, time_index, T
    with open('cache.pkl', 'wb') as f:
        pickle.dump(data, f)

    return data

def replace_underscores(X):
    new_X = []
    for d in X:
        new_d = {}
        for k, v in d.items():
            new_k = k.replace('_', '.')
            new_d[new_k] = v
        new_X.append(new_d)
    return new_X

def dict_to_2d_list(X_dict):

    console.log('Converting into X into 2D list ...')

    X_list = []
    for x_dict in X_dict:
        x_list = list(x_dict.keys())
        X_list.append(x_list)
    return X_list

def is_ip_address(url):
    try:
        ipaddress.ip_address(url)
        return True
    except ValueError:
        return False
    
def create_features_df(X_2D):

    high_risk_tlds = [".sbs", ".cyou", ".quest", ".autos", ".top", ".makeup", ".skin", ".cloud", ".best", ".icu", ".cfd", ".shop", ".homes", ".xyz", ".bio", ".monster", ".ink", ".life", ".pics", ".bd", ".pro", ".py", ".live", ".th", ".run", ".ltd", ".store", ".today", ".id", ".buzz", ".club", ".space", ".website", ".pk", ".tz", ".am", ".site", ".click", ".mk", ".cc", ".ng", ".beauty", ".link", ".cn", ".lk", ".pe", ".tr", ".su", ".ke", ".asia"]
    #From netcraft: https://trends.netcraft.com/cybercrime/tlds

    console.log("Extracting features ...")

    def calculate_number_of_urls(urls):
        return len(urls)
    
    def calculate_average_length(urls):
        return sum(len(url) for url in urls) / len(urls)
    
    def calculate_letters_numbers_ratio(urls):
        total_ratio = 0
        count = 0

        for url in urls:
            letters = sum(char.isalpha() for char in url)
            numbers = sum(char.isdigit() for char in url)

            if letters + numbers > 0:
                ratio = letters / (letters + numbers)
                total_ratio += ratio
                count += 1

        average_ratio = total_ratio / count if count != 0 else 0
        return average_ratio
    
    def parse_urls(X_2D):
        
        parsed_urls = {}

        for urls in X_2D:
            for url in urls:
                if url not in parsed_urls:
                    try:
                        parsed_urls[url] = urlparse(url)
                    except ValueError:
                        parsed_urls[url] = None

        return parsed_urls
    
    parsed_urls = parse_urls(X_2D)
    
    def calculateRatioOfValidUrls(urls):
        total = len(urls)
        validTotal = sum(parsed_urls[url] is not None for url in urls)
        ratio = validTotal / total if total != 0 else 0
        return ratio
    
    def calculate_average_domain_length(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        total = sum(len(parsed_urls[url].netloc) for url in valid_urls)
        average = total / len(valid_urls) if valid_urls else 0
        return average

    def calculate_average_path_length(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        total = sum(len(parsed_urls[url].path) for url in valid_urls)
        average = total / len(valid_urls) if valid_urls else 0
        return average
    
    def calculate_average_subdomains(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        total_subdomains = 0

        for url in valid_urls:
            netloc = parsed_urls[url].netloc
            parts = netloc.split('.')
            # Subtract 2 to exclude main domain and TLD. If there's www or any other prefix, it will be counted.
            num_subdomains = max(0, len(parts) - 2)
            total_subdomains += num_subdomains

        average_subdomains = total_subdomains / len(valid_urls) if valid_urls else 0
        return average_subdomains

    def calculate_high_risk_tld_count(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        high_risk_tld_count = 0

        for url in valid_urls:
            netloc = parsed_urls[url].netloc
            tld = '.' + netloc.split('.')[-1]  # get the last part after the last dot, which should be the TLD
            if tld in high_risk_tlds:
                high_risk_tld_count += 1

        return high_risk_tld_count
    
    def calculate_avg_count_for_keyword(urls, keyword):
        valid_urls = [url for url in urls if parsed_urls[url]]
        keyword_counts = [url.count(keyword) for url in valid_urls]

        average = sum(keyword_counts) / len(valid_urls) if valid_urls else 0

        return average

    def calculate_avg_login_count(urls):
        return calculate_avg_count_for_keyword(urls, 'login')

    def calculate_avg_account_count(urls):
        return calculate_avg_count_for_keyword(urls, 'account')

    def calculate_avg_verify_count(urls):
        return calculate_avg_count_for_keyword(urls, 'verify')

    def calculate_avg_client_count(urls):
        return calculate_avg_count_for_keyword(urls, 'client')

    def calculate_avg_admin_count(urls):
        return calculate_avg_count_for_keyword(urls, 'admin')

    def calculate_avg_server_count(urls):
        return calculate_avg_count_for_keyword(urls, 'server')


    def calculate_number_of_ip_addresses(urls):
        total = sum(is_ip_address(parsed_urls[url].netloc) for url in urls if parsed_urls[url])
        return total

    def calculate_https_ratio(urls):
        valid_urls = [url for url in urls if parsed_urls[url]]
        https = sum(parsed_urls[url] and parsed_urls[url].scheme == "https" if parsed_urls[url] is not None else False for url in urls)
        ratio = https / len(valid_urls) if valid_urls != 0 else 0
        return ratio
    
    # Define a function to calculate the number of dots and its average
    def calculate_avg_num_dots(urls):
        num_dots = sum(url.count('.') for url in urls)
        avg_num_dots = num_dots / len(urls)
        return avg_num_dots

    # Define a function to calculate the number of directories and its average
    def calculate_avg_num_dirs(urls):
        num_dirs = sum(url.count('/') for url in urls)
        avg_num_dirs = num_dirs - (2*len(urls)) / len(urls)
        #We remove the two initals slashes // from the scheme from the count
        return avg_num_dirs
    
    # Define a function to calculate the number of "#" occurrences and its average
    def calculate_avg_num_fragments(urls):
        num_fragmnets = sum(url.count('#') for url in urls)
        avg_num_fragments = num_fragmnets / len(urls)
        return avg_num_fragments

    # Define a function to calculate the number of "@" occurrences and its average
    def calculate_avg_num_at(urls):
        num_ats = sum(url.count('@') for url in urls)
        avg_num_ats = num_ats / len(urls)
        return avg_num_ats

    # Define a function to calculate the number of "%" [Encoded] occurrences and its average
    def calculate_avg_num_encoded(urls):
        num_encoded = sum(url.count('%') for url in urls)
        avg_num_encoded = num_encoded / len(urls)
        return avg_num_encoded
    
    # Define a function to calculate the number of queries occurrences and its average
    def calculate_avg_num_queries(urls):
        num_queries = sum(url.count('?') for url in urls)
        avg_num_queries = num_queries / len(urls)
        return avg_num_queries


    # Define a function to calculate the number of "=" occurrences and its average
    def calculate_avg_num_equal(urls):
        num_equals = sum(url.count('=') for url in urls)
        avg_num_equals = num_equals / len(urls)
        return avg_num_equals
    
    def calculate_avg_entropy(urls):
        
        total = 0

        for url in urls:
            # Count the frequency of each symbol in the string
            symbol_counts = Counter(url)

            # Calculate the probabilities of each symbol
            total_symbols = len(url)
            probabilities = [count / total_symbols for count in symbol_counts.values()]

            # Calculate the entropy using the formula
            entropy = -sum(p * math.log2(p) for p in probabilities)

            total += entropy
        
        return total / len(urls)
    
    def count_url_shortened(urls):

        shortened_services = ['bit.ly', 'goo.gl', 't.co', 'ow.ly', 'tinyurl', 'is.gd', 'v.gd', 'clck.ru', 'rebrand.ly', 'soo.gd', 'cutt.ly', 'u.nu', 'fur.ly', 'adcrun.ch', 'shorl.com', 'x.co', 'bc.vc', 'adf.ly', 'ouo.io', 'tiny.cc', 'clk.im', 'tr.im', 'short.am', 'rb.gy', 'shorte.st', 'shorturl.at', 'lnkd.in', 'viid.me', 'adfly', 'qr.net', 'bit.do', 'zz.gd', 'adcraft.co', 'moourl.com', 'vzturl.com', 'met.bz', 'filoops.info', 'ity.im', 'short.cm', 'linkshrink.net', 'shorten.to', 'link.tl']
        count = 0

        for url in urls:

            parsed_url = parsed_urls[url]
        
            if parsed_url:
                domain = parsed_url.netloc.lower()
                if domain in shortened_services:
                    count += 1

        return count

    def count_number_of_ports(urls):
        
        count = 0

        for url in urls:
            parsed_url = parsed_urls[url]
            try: 
                if parsed_url and parsed_url.port is not None:
                        port = int(parsed_url.port)
                        count += 1
            except ValueError:
                continue

        return count
    
    # Define a function to calculate the number of digits and its average
    def calculate_avg_num_digits(urls):
        num_digits = sum(url.count(char) for url in urls for char in '0123456789')
        avg_num_digits = num_digits / len(urls)
        return avg_num_digits
    
    def calculate_avg_num_letters(urls):
        num_letters = sum(url.count(char) for url in urls for char in 'abcdefghiklmnopqrstuvwxyz')
        avg_num_letters = num_letters / len(urls)
        return avg_num_letters
    
            
    # Create a DataFrame with the URLs
    df = pd.DataFrame({'URLs': X_2D})

    df["Number of URLs"] = df['URLs'].apply(calculate_number_of_urls)
    console.log("Number of URLs calculation completed.")

    df["Number of IPs addresses"] = df['URLs'].apply(calculate_number_of_ip_addresses)
    console.log("Number of IP addresses calculation completed.")

    df["Average Length of URLs"] = df['URLs'].apply(calculate_average_length)
    console.log("Average Length of URLs calculation completed.")

    df["Ratio Valid URLS"] = df['URLs'].apply(calculateRatioOfValidUrls)
    console.log("Ratio Valid URLs calculation completed.")

    df["Average Domains Length"] = df['URLs'].apply(calculate_average_domain_length)
    console.log("Average Domains Length calculation completed.")

    df["Average Paths Length"] = df['URLs'].apply(calculate_average_path_length)
    console.log("Average Paths Length calculation completed.")

    df["Average Subdomain Count"] = df['URLs'].apply(calculate_average_subdomains)
    console.log("Average Subdomain Count calculation completed.")

    df["High Risk TLD Count"] = df['URLs'].apply(calculate_high_risk_tld_count)
    console.log("High Risk TLD Count calculation completed.")

    df["Average Login Count"] = df['URLs'].apply(calculate_avg_login_count)
    console.log("Average Login Count calculation completed.")

    df["Average Account Count"] = df['URLs'].apply(calculate_avg_account_count)
    console.log("Average Account Count calculation completed.")

    df["Average Verify Count"] = df['URLs'].apply(calculate_avg_verify_count)
    console.log("Average Verify Count calculation completed.")

    df["Average Client Count"] = df['URLs'].apply(calculate_avg_client_count)
    console.log("Average Client Count calculation completed.")

    df["Average Admin Count"] = df['URLs'].apply(calculate_avg_admin_count)
    console.log("Average Admin Count calculation completed.")

    df["Average Server Count"] = df['URLs'].apply(calculate_avg_server_count)
    console.log("Average Server Count calculation completed.")

    df["HTTPS ratio"] = df['URLs'].apply(calculate_https_ratio)
    console.log("HTTPS ratio calculation completed.")

    df["Average Number of Dots"] = df['URLs'].apply(calculate_avg_num_dots)
    console.log("Average Number of Dots calculation completed.")

    df["Average Number of Paths"] = df['URLs'].apply(calculate_avg_num_dirs)
    console.log("Average Number of Directories calculation completed.")

    df["Average Number of At Symbols"] = df['URLs'].apply(calculate_avg_num_at)
    console.log("Average Number of At Symbols calculation completed.")

    df["Average Number of Fragments"] = df['URLs'].apply(calculate_avg_num_fragments)
    console.log("Average Number of fragments calculation completed.")

    df["Average Number of Encoded Characters"] = df['URLs'].apply(calculate_avg_num_encoded)
    console.log("Average Number of encoded characters calculation completed.")

    df["Average Number of Queries"] = df['URLs'].apply(calculate_avg_num_queries)
    console.log("Average Number of queries calculation completed.")

    df["Average Entropy"] = df['URLs'].apply(calculate_avg_entropy)
    console.log("Average Entropy calculation completed.")

    df["Count of Shortened Services"] = df['URLs'].apply(count_url_shortened)
    console.log("Number of shortened services used calculated.")

    df["Number of Ports"] = df['URLs'].apply(count_number_of_ports)
    console.log("Number of ports used calculated.")

    df["Number of Digits"] = df['URLs'].apply(calculate_avg_num_digits)
    console.log("Avg Number of digits used calculated.")

    df["Number of Letters"] = df['URLs'].apply(calculate_avg_num_letters)
    console.log("Avg Number of letters used calculated.")

    df["Letters/Numbers Ratio Average"] = df['URLs'].apply(calculate_letters_numbers_ratio)
    console.log("Letters/Numbers Ratio calculation completed.")

    #NOT USED from here

    # df["Average Number of Hyphens"] = df['URLs'].apply(calculate_avg_num_hyphen)
    # console.log("Average Number of Hyphens calculation completed.")

    # df["Average Number of Dollar Signs"] = df['URLs'].apply(calculate_avg_num_dollar)
    # console.log("Average Number of Dollar Signs calculation completed.")

    # df["Average Number of Ampersands"] = df['URLs'].apply(calculate_avg_num_ampersand)
    # console.log("Average Number of Ampersands calculation completed.")

    # df["Average Number of Equal Signs"] = df['URLs'].apply(calculate_avg_num_equal)
    # console.log("Average Number of Equal Signs calculation completed.")

    # df["Total Length of URLs"] = df['URLs'].apply(calculate_total_length_of_urls)
    # console.log("Total Length of URLs calculation completed.")

    # df['Average TLD length'] = df['URLs'].apply(calculate_avg_tld_length)
    # console.log("Average TLD length calculated.")

    # df["Total Paths Length"] = df['URLs'].apply(calculate_total_path_length)
    # console.log("Total Paths Length calculation completed.")

    # df["Total Domains Length"] = df['URLs'].apply(calculate_total_domain_length)
    # console.log("Total Domains Length calculation completed.")

    # df["Total Length of URLs"] = df['URLs'].apply(calculate_total_length_of_urls)
    # console.log("Total Length of URLs calculation completed.")

    # Drop the 'URLs' and 'Average Length' columns from the DataFrame
    df = df.drop(['URLs'], axis=1)

    return df

def filterData(X, y, filter_key):

    console.log(f"Filtering data with filter_key: ({filter_key})....")
    filtered_X = []
    filtered_y = []
    prefix_len = len(filter_key)

    for i in range(len(X)):
        if any(k.startswith(filter_key) for k in X[i]):
            filtered_dict = {k[prefix_len:]: v for k, v in X[i].items() if k.startswith(filter_key)}
            filtered_X.append(filtered_dict)
            filtered_y.append(y[i])

    filtered_X = replace_underscores(filtered_X)

    return filtered_X, filtered_y

def evaluate_classifier(clf, X_test, y_test):
    """
    Evaluates a trained classifier on a given testing set.

    Args:
        clf: The trained classifier.
        X_test: The testing set features.
        y_test: The testing set labels.

    Returns:
        A dictionary of the performance metrics including testing accuracy, false positive rate,
        false negative rate, and F1 score.
    """

    console.log('Evaluating Classifier...')

    # Test the classifier on the testing set
    y_pred = clf.predict(X_test)
    accuracy = clf.score(X_test, y_test)
    f1score = f1_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    false_positive_rate = fp / (fp + tn)
    false_negative_rate = fn / (fn + tp)

    print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score)

    results = {
        "accuracy": accuracy,
        "false_positive_rate": false_positive_rate,
        "false_negative_rate": false_negative_rate,
        "f1_score": f1score,
    }

    return results

def print_evaluation_metrics(accuracy, false_positive_rate, false_negative_rate, f1score):
    """
    Prints the evaluation metrics of a classifier.

    Args:
        accuracy (float): The testing accuracy of the classifier.
        false_positive_rate (float): The false positive rate of the classifier.
        false_negative_rate (float): The false negative rate of the classifier.
        f1score (float): The F1 score of the classifier.
    """
    print(f'Testing accuracy: {accuracy}')
    print(f'False Positive rate: {false_positive_rate}')
    print(f'False Negative rate: {false_negative_rate}')
    print(f'F1 Score: {f1score}')

def train_linear_svc(vectorised_X, y_vectorised, feature_names):

    console.log('Training SVC Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorised_X, y_vectorised, test_size=0.1, random_state=42)

    # Train the LinearSVC classifier
    clf = LinearSVC(max_iter=10000)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    console.log('Finding most important features..')

    coef = clf.coef_[0]
    top_indices = np.argsort(np.abs(coef))[::-1][:10]
    top_features = [feature_names[i] for i in top_indices]

    print(f"Top 10 features: {top_features}")
    print(f"Feature weights: {coef[top_indices]}")

    return clf, X_train, X_test

def train_random_forest(vectorised_X, y_vectorised, feature_names, n_jobs=-1):

    console.log('Training Random Forest Classifier...')

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(vectorised_X, y_vectorised, test_size=0.1, random_state=21)

    # Train the Random Forest classifier
    clf = RandomForestClassifier(n_jobs=n_jobs, random_state=42)
    clf.fit(X_train, y_train)

    # Test the classifier on the testing set
    evaluate_classifier(clf, X_test, y_test)

    console.log('Finding most important features..')

    feature_importances = clf.feature_importances_
    top_indices = np.argsort(feature_importances)[::-1][:10]
    top_features = [feature_names[i] for i in top_indices]

    print(f"Top 10 features: {top_features}")
    print(f"Feature importances: {feature_importances[top_indices]}")

    #Additional less helpful features
    less_helpful_indices = np.argsort(feature_importances)[:10]
    less_helpful_features = [feature_names[i] for i in less_helpful_indices]

    print(f"Less helpful features: {less_helpful_features}")
    print(f"Feature importances: {feature_importances[less_helpful_indices]}")

    return clf, X_train, X_test

def subsample_program(X, y, fraction):
    
    console.log("Subsampling data ...")

    split_index = int(len(X) * fraction)

    X_filtered = X[:split_index]
    y_filtered = y[:split_index]

    return X_filtered, y_filtered

with open("filtered_data.pickle", "rb") as f:
    console.log("Loaded filtered data from cache")
    X_filtered, y_filtered = pickle.load(f)

# Subsample data
X_filtered, y_filtered = subsample_program(X_filtered, y_filtered, 1)

X_2D = dict_to_2d_list(replace_underscores(X_filtered))

X_df = create_features_df(X_2D)

feature_names = X_df.columns.tolist()

# svc_clf = train_linear_svc(X_df, y_filtered, feature_names)

# with open('lexical_f_svc_clf.pkl', 'wb') as fid:
#     pickle.dump(svc_clf, fid)

# random_forest_clf, X_train, X_test = train_random_forest(X_df, y_filtered, feature_names)

# with open('lexical_f_rf_clf.pkl', 'wb') as fid:
#     pickle.dump(random_forest_clf, fid)

